<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Glow: Glow optimization passes</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Glow
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Glow optimization passes </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This document describes the target-independent optimizations performed by the graph and IR optimizers and some implementation details.</p>
<h3>Overview</h3>
<p>Glow has two different optimizers: the graph optimizer and the IR optimizer.</p>
<p>The graph optimizer performs optimizations on the graph representation of a neural network model. The nodes of the graph usually represent more coarse grained operations than those represented by the IR instructions. These operations also do not explicitly represent memory allocations and buffers.</p>
<p>The IR optimizer performs a number of optimizations on the IR representation of a neural network model.</p>
<p>The optimizations have two major objectives. One is to improve the performance of training and inference steps. The other one is to reduce the memory consumption during the execution of neural network models.</p>
<p>It is worth mentioning that performing optimizations reducing memory consumption is easier at the IR level, because memory allocations and deallocations are explicitly represented in the IR, whereas they are not explicit in the graph representation.</p>
<h3>Set of supported graph optimizations</h3>
<p>Below you can see the list of currently supported graph optimizations:</p><ul>
<li><p class="startli">Dead code elimination (DCE)</p>
<p class="startli">This optimization removes computations whose results or side-effects are not used.</p>
</li>
<li><p class="startli">Optimization of transpose nodes</p>
<p class="startli">This optimization combines multiple consecutive transpose nodes into a single node, eliminates identity transpose nodes, and optimizes transpose nodes into reshape nodes when they actually move no data.</p>
</li>
<li><p class="startli">Sinking of transpose operations below other operations</p>
<p class="startli">This optimization sinks transposes below such operations like a batch normalization, RELU, sigmoid, ChannelShuffle, etc. By doing this, many transpose operations are brought closer to each other and it creates more opportunities for elimination of transpose operations.</p>
</li>
<li><p class="startli">Pool operations optimization</p>
<p class="startli">This optimization swaps the order of Relu-&gt;MaxPool, to perform the RELU operation on a smaller tensor. This optimization is not a major performance win. The RELU operation takes a small fraction of the time, and reordering the nodes does not provide a lot of performance wins. However, reordering the buffers allows us to reuse the memory buffer of the pool operation and potentially save memory.</p>
</li>
<li><p class="startli">Optimizing of regression nodes in the inference mode</p>
<p class="startli">In inference mode Regression nodes simply forward their inputs.</p>
</li>
<li><p class="startli">Optimization of concat nodes</p>
<p class="startli">This optimization merges multiple consequent concat nodes into a single concat node.</p>
</li>
<li><p class="startli">Common sub-expression elimination</p>
<p class="startli">This optimization performs a classic CSE with a goal of avoiding of any results that were computed already.</p>
</li>
<li><p class="startli">Optimization of ReduceMean nodes</p>
<p class="startli">This optimization performs substitions of ReduceMean with AvgPool node if the reduce parameters are suitable: input is 4D with last two dimensions to be reduced.</p>
</li>
</ul>
<h4>Quantization specific optimizations</h4>
<p>Majority of the common optimizations above can be used on a quantized graph. But in addition to those there are quantization specific optimizations:</p><ul>
<li><p class="startli">Quantize(Dequantize(X)) -&gt; RescaleQuantized(X)</p>
<p class="startli">If the Quantize-Dequantize sequence does not change the type then this sequence is simply dropped without adding nop RescaleQuantized node. If Dequantize node has an input type that is different from the Quantize node output type then a RescaleQuantized node replaces Quantize-Dequantize.</p>
</li>
<li><p class="startli">Dequantize(Quantize(X))</p>
<p class="startli">A sequence of Dequantize(Quantize(X)) is a nop transformation and can be completely removed.</p>
</li>
<li><p class="startli">RescaleQuantized(RescaleQuantized(X)</p>
<p class="startli">A sequence of RescaleQuantized operators can be replaced by just a single RescaleQuantized.</p>
</li>
<li><p class="startli">Constants optimization</p>
<p class="startli">Constants which have single use could be quantized at the optimization phase. This optimization replaces Quantize(Constant) with just a Constant with updated quantized weights based on the quantization parameters from the Quantize node.</p>
</li>
<li><p class="startli">RescaleQuantized(Max(X,Y)) -&gt; Max(RescaleQuantized(X), RescaleQuantized(Y))</p>
<p class="startli">It's OK to rescale the operands because even if the output range is smaller then truncation would have happened during the rescaling. On values that are outside of the range, we just move the truncation to a different location.</p>
</li>
<li><p class="startli">Combine RescaleQuantized operator up into the operation</p>
<p class="startli">There are a number of operations which can operate on varying quantized parameters for the output type. It's safe to just merge RescaleQuantized node into the operator itself if operator supports this, e.g., add, mul, etc.</p>
<p class="startli">This optimization can be applied to:</p><ul>
<li>Add</li>
<li>Sub</li>
<li>Mul</li>
<li>Div</li>
<li>Min</li>
<li>Max</li>
<li>Convolution</li>
<li>Splat</li>
</ul>
</li>
<li><p class="startli">Combine RescaleQuantized operator down into the operation</p>
<p class="startli">This optimization allows eliminating redundant rescale operations when the next operation supports quantized inputs of different scales and offsets, e.g., normal arithmetic operations: Add, Sub, Mul, Div, Min, Max.</p>
</li>
<li><p class="startli">Sinking RescaleQuantized operator below other operators</p>
<p class="startli">This optimization sinks RescaleQuantized node below such operations as slice, reshape, transpose, etc. By doing this, many RescaleQuantized operators are brought closer to each other, and it creates more opportunities for elimination of RescaleQuantized operations.</p>
</li>
<li><p class="startli">RescaleQuantized(Quantize(X)) -&gt; Quantize(X)</p>
<p class="startli">A sequence of Quantize operation followed by RescaleQuantized operation is replaced by a single Quantize operation with the proper quantization parameters based on the RescaleQuantized operation.</p>
</li>
<li><p class="startli">Eliminate Max operation in Max(Splat(X), someOperand) or Max(someOperand, Splat(X))</p>
<p class="startli">Splat and Max operations can be completely eliminated if Splat value cannot impact the result of the Max operation. For example, Max and Splat are removed if Splat value is smaller than the smallest possible value from the other operand. Smallest possible value from the operand can be calculated based on the quantization parameters which represent quantization range [min, max] in fp32.</p>
</li>
</ul>
<h4>Configuring a graph optimization pipeline</h4>
<p>The graph optimizations listed above are each formulated as a FunctionPass, which is run on a Function (graph). A series of FunctionPasses along with how to configure each (via a <code>FunctionPassConfig</code>) is what constitutes a pipeline, which is passed into a PassManager, the driver of performing passes. A pipeline is simply a vector of <code>FunctionPassConfig</code>s. <code>FunctionPassConfig</code> is a class made up of:</p>
<ul>
<li><code>FunctionPassID</code>: An ID corresponding to a specific FunctionPass. For example, <code>FuncionPassID::OptimizeArithmeticNodes</code>. Default: <code>EmptyPass</code> (no-op pass).</li>
<li><code>ConvergenceMode</code>: An enum corresponding to whether this FunctionPass should be run a single time (OnePass) or repeatedly until a fixed point is reached (<code>UntilFixedPoint</code>). Default: <code>OnePass</code>.</li>
<li><code>DCERequiredMode</code>: An enum representing whether DCE is required before a pass is run (<code>BeforePass</code>), or not at all (<code>None</code>). Running DCE is often required in order to make sure the number of users for each Node is up to date. Default: <code>BeforePass</code>.</li>
<li><code>EnabledCompilationModes</code>: A set of <code>CompilationMode</code>s representing which mode(s) the pass should run under. Default: <code>{Infer, Train}</code>.</li>
</ul>
<h3>Set of supported IR optimizations</h3>
<p>Below you can see the list of currently supported optimizations:</p>
<ul>
<li><p class="startli">Peephole optimizations</p>
<p class="startli">These are small, local optimizations that look for specific sequences of instructions and replace them with more efficient sequences of instructions.</p>
</li>
<li><p class="startli">Dead store elimination (DSE)</p>
<p class="startli">This optimization removes stores into weights or allocations if it can prove that the results of these stores are never going to be used.</p>
</li>
<li><p class="startli">Deallocations hoisting</p>
<p class="startli">This optimization tries to place the buffer deallocation instructions right after the last use of a buffer. Doing so reduces the lifetime of the buffer and makes the freed memory available for the allocation of other buffers. It improves the memory consumption.</p>
</li>
<li><p class="startli">Allocations sinking</p>
<p class="startli">This optimization tries to place the buffer allocation instructions right before the first use of a buffer. Doing so reduces the lifetime of the buffer and makes the unused memory available for the allocation of other buffers. It improves the memory consumption.</p>
</li>
<li><p class="startli">Dead allocations removal</p>
<p class="startli">This optimization finds and removes allocations that are just allocated and deallocated, but are never used. Such situations may happen e.g. after performing other allocations. Performing this optimization improves the memory consumption.</p>
</li>
<li><p class="startli">Making weights constant</p>
<p class="startli">This optimization marks weights that are never mutated as constant. This may allow for placing such weights in a read only memory segments and share it between simultaneous executions of the same neural network model.</p>
</li>
<li><p class="startli">Sharing of buffers</p>
<p class="startli">The purpose of this optimization is to reduce the memory usage by reusing the memory buffers as much as possible. The overall idea is that it is fine to combine storage for two live intervals if they do not overlap. Typically, two live intervals are considered as candidates for sharing if they occur in the same instruction.</p>
</li>
<li><p class="startli">Stacking of data-parallel operations</p>
<p class="startli">Stacking tries to combine multiple data parallel (i.e. element-wise) operations that work with the same shape of tensors into a single kernel.</p>
<p class="startli">Executing such a kernel should be in theory more efficient than executing those operations sequentially one after the other, because such a combined kernel exposes a better cache locality.</p>
<p class="startli">The stacked kernels should provide even more advantages on GPUs, because they reduce the number of kernel threads launches, which are rather expensive operations. </p>
</li>
</ul>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
