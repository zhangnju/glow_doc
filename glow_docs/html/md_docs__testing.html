<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Glow: Testing the Glow compiler</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Glow
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Testing the Glow compiler </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The Glow test suite contains four major categories: unit tests, regression tests, example programs, and the model loader. Unit tests are the small tests that stress specific parts of the compiler. These tests are added to the compiler when developing a feature. For example, we train a number of small network and perform a gradient check on the operators. We also compile networks to IR and look for specific patterns. Regression tests are tests that are added when we fix bugs. Both regression tests and feature tests are found under the "test/" directory. To run the feature and regression tests run "ninja test".</p>
<h2>Example test suites.</h2>
<p>We rely on external test suites to test the compiler. We use the data sets CIFAR10 and MNIST (located in the "example/" directory) to test the correctness of the whole system. The script under 'utils/' download and extract the data set.</p>
<h2><a class="el" href="struct_model.html">Model</a> Loader</h2>
<p>We test the correctness of the Glow implementation by loading Caffe2 and ONNX models and executing them end-to-end.</p>
<h3>Image Classification</h3>
<p>The program <code>image-classifier</code> loads a model, a png file, and runs a single pass of inference. If everything goes right the output of the program is identical to the output of the original (Caffe2 or ONNX) model. Unfortunately, the models do not usually describe what the input format should be. Should the pixels be between zero and one, or negative 128 to positive 128? The user needs to be aware of these things when running the models. The script in the directory 'utils/' downloads a number of pre-trained networks that we can use for testing.</p>
<p>The Glow build scripts copy a few sample images and a run script that tests the <code>image-classifier</code> program. The script can be executed with the command:</p>
<div class="fragment"><div class="line">build$./tests/images/run.sh</div></div><!-- fragment --><h4>Calculating Top-1 and Top-5 Accuracy</h4>
<p>The script <code>imagenet_topk_accuracy_driver.py</code> located in the <code>utils/</code> directory can be used to calculate Top-1 and Top-5 accuracy. It can be run via a command like the following:</p>
<div class="fragment"><div class="line">python utils/imagenet_topk_accuracy_driver.py --batch-size=10 --validation-images-dir=${PATH_TO_IMAGES} --image-classifier-cmd=&quot;${PATH_TO_IMAGE_CLASSIFIER_BINARY} -image-mode=0to1 -m=${PATH_TO_RESNET50_PROTOS_DIR} -model-input-name=gpu_0/data -backend=CPU -topk=5 -&quot;</div></div><!-- fragment --><p>Note that the <code>--image-classifier-cmd</code> must include <code>-topk=5</code> for printing the Top-5 labels, and <code>-</code> to run in streaming mode.</p>
<p>The script expects the directory passed in via <code>--validation-images-dir</code> to contain subdirectories alphabetically ordered in order of increasing label. For example, for Imagenet with 1000 labels, subdirectories could be listed as <code>label000/, label001/, ... , label999/</code>, where <code>label000/</code> contains all images that should be classified with label 0.</p>
<p>The script can be used to resize and center crop images to 224x224 via <code>--resize-input-images</code>. This resize and center cropping can be done by itself via <code>--only-resize-and-save</code>, improving execution time of calculating Top-k accuracy more than once (this saves the processed images to <code>validation_images_dir/processed/</code>).</p>
<h3>Text Translation</h3>
<p>The program <code>text-translator</code> loads a text translation model, reads a line from stdin in a source language, and then prints the translation to the command line in the destination language. The text translation model should be specified by a directory via <code>-m</code>, containing the source and destination dictionaries (<code>src_dictionary.txt</code> and <code>dst_dictionary.txt</code>), as well as the protobuf files for the model. A backend can be optionally specified, just like for the <code>image-classifier</code>.</p>
<div class="fragment"><div class="line">$ ./bin/text-translator -m en2gr -backend=CPU</div><div class="line"></div><div class="line">Enter a sentence in English to translate to German: My favorite sport is basketball .</div><div class="line">mein Lieblingssport ist Basketball .</div></div><!-- fragment --><p>This program expects a sequence-to-sequence model with beam search. Because Glow currently does not support models that contain control flow (e.g. the <a href="https://caffe2.ai/docs/operators-catalogue.html#recurrentnetwork">RecurrentNetwork operator from Caffe2</a>), the input model must be unrolled to some maximum input and output length. These can be specified on the command line via <code>-max-input-len</code> and <code>-max-output-len</code>. Additionally, the beam search size can be specified via <code>-beam-size</code>. The default options for the <code>text-translator</code> match those for the en2gr model currently downloaded via <code>utils/download_datasets_and_models.py</code> (<code>-max-input-len=10</code>, <code>-max-output-len=14</code>, <code>-beam-size=6</code>).</p>
<h2>Caffe2 and ONNX Models</h2>
<p><a class="el" href="struct_model.html">Model</a> loader programs (e.g. <code>image-classifier</code> and <code>text-translator</code>) load pre-trained models from protobuf file (either Caffe2 or ONNX). These pre-trained models are downloaded via <code>download_datasets_and_models.py</code> script located in <code>utils/</code>.</p>
<p>There is a more general way to run a pre-trained model, not related to images. The <code>model-runner</code> program loads and runs a self-contained model, i.e. a model, which has all its inputs initialized inside itself and does not ask for user's input.</p>
<h3>Train and Save Caffe2 Models</h3>
<p>The <code>caffe2_train_and_dump_pb.py</code> script in <code>utils/</code> allows the user to define their own models and input training set in Caffe2, and then dumps the network and weights to protobuf files (the network structure in <code>predict_net.pb/pbtxt</code> and the pre-trained weights in <code>init_net.pb</code>). Right now it trains either LeNet on MNIST; an MLP is also available and can be used by setting <code>USE_LENET_MODEL = False</code>. This script is heavily based on the MNIST.py tutorial from Caffe2.</p>
<h3>Run the pre-trained Caffe2 Models using Caffe2</h3>
<p>The <code>caffe2_pb_runner.py</code> script in <code>utils/</code> loads and runs a pre-trained model using the protobuf files saved using <code>caffe2_train_and_dump_pb.py</code>. This can be used to compare the output from Glow to Caffe2. Its usage is similar to running the <code>image-classifier</code>, which is found in the <code>run.sh</code> script in <code>tests/images/</code>. For example, the following command will run the pre-trained resnet50 model using Caffe2:</p>
<div class="fragment"><div class="line">python caffe2_pb_runner.py -i [location_of_image] -d resnet50</div></div><!-- fragment --><h2>Integrated Testing</h2>
<p>Glow also comes with tests integrated with the build environment for our command line tools. We run those tests as part of our continuous integration (CI).</p>
<p>Run them as part of your local build using the following </p><div class="fragment"><div class="line">cmake -G Ninja &lt;glow_src&gt;  -DCMAKE_BUILD_TYPE=Release \</div><div class="line">      -DCMAKE_PREFIX_PATH=/usr/local/opt/llvm         \</div><div class="line">      -DGLOW_MODELS_DIR=&lt;downloaded_c2_models&gt;</div></div><!-- fragment --><p> Followed by </p><div class="fragment"><div class="line">ninja check_expensive</div></div><!-- fragment --><p>Note: <code>ninja check_expensive</code> runs all of the tests that <code>ninja check</code> runs plus any tests that have been marked as EXPENSIVE using add_glow_test(EXPENSIVE ...) such as the integration tests.</p>
<p>Note: The difference between <code>ninja test</code> and <code>ninja check</code> is that <code>ninja check</code> makes sure the build dependencies are current before running the tests. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
