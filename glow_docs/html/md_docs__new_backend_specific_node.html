<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Glow: New Backend-Specific Node</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Glow
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">New Backend-Specific Node </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This document describes how to add new backend-specific node to make further optimization on certain backend.</p>
<h3>Graph Optimization</h3>
<p>Glow has two levels of IR. The high-level IR is a dataflow node-based graph. There are several strategies to make transformation on high-level IR to achieve better performance on certain backend. Two of these strategies will introduce new backend-specific node.</p>
<h4>Operator Fusion</h4>
<p>There are mainly two advantages of operator fusion:</p>
<ul>
<li>Good locality and less memory access</li>
<li>Reduce kernel launch time in GPU and specialized accelerators</li>
</ul>
<p>In TensorRT, the convolution, bias and ReLU layers of various sizes can be combined into a single kernel called CBR. A simple analogy is making three separate trips to the supermarket to buy three items with close relation versus buying all three in a single trip, which reduce the time of seeking items and starting trips.</p>
<h4>Data Layout Transformation</h4>
<p>Actually, a Tensor is a view of a block of memory. Besides a pointer to the memory, we also have to get some other descriptions of this block of memory, such as shape, stride and layout.</p>
<p>Different layout leads to different implementation of the operator kernel on certain backend. For example, in the subgraph from ResNet50, a <code>CPUConvDKKC8</code> node with memory layout modified for efficient SIMD access is introduced to optimized for CPU backend. Please refer to "5.3 Use Case: Optimizing Resnet50 for the CPU" in <a href="https://arxiv.org/abs/1805.00907">Glow paper</a>.</p>
<p>We should take both fast operator kernel implementation and extra potential layout transformation into consideration to get better performance.</p>
<h3>Steps</h3>
<p>This section is about (1) adding new backend-specific nodes and corresponding instructions, (2) how to utilize the APIs to add these backend-specifics nodes to the graph, and (3) how to have your backend correctly handle this new corresponding backend-specific instruction that is IRGen'd from your backend-specific Node.</p>
<p>Here are mainly three steps to add a new backend-specific node in Glow:</p>
<ol type="1">
<li>Add a backend-specific Node <code>FusedAB</code> to <code>XSpecificNodes.h</code>, and a corresponding backend-specific Instruction <code>FusedAB</code> to <code>XSpecificInstrs.h</code>. Note that the <code>FusedABInst</code> needs to be marked with <code>autoIRGen()</code> so that the node is automatically IRGen'd to the instruction, as we currently do not support backend-specific IRGen.</li>
<li>Add logic to <code>XBackend::transformPostLowering()</code> that looks for the pattern of nodes you want to fuse (<code>A</code> with a single use by <code>B</code>), and replaces all uses of the result of B with the new backend-specific <code>FusedABNode</code>.</li>
<li>Have your backend <code>X</code> implement <code>FusedABInst</code>. For example, for the OpenCL backend, this would mean adding a case to enqueue a kernel for the <code>FusedABInst</code> to <code>OpenCLFunction::execute()</code>, and then adding the corresponding kernel in <code>kernels.cl</code>.</li>
</ol>
<h3>Examples</h3>
<h4>Operator Fusion for ReLU in CPU</h4>
<p>ReLU is max between zero and the input value. Glow lowers <code>ReLUNode</code> to two basic low-level linear algebra operator nodes, <code>SplatNode</code> and <code>MaxNode</code>. The <code>SplatNode</code> first fills a Tensor with zero, and <code>MaxNode</code> compare <code>Input</code> with the filling Tensor. We can fuse these two operations which work with the same shape of tensors into a single kernel.</p>
<p>Please refer to the document in <a href="https://github.com/pytorch/glow/blob/master/docs/Backends.md#backend-specific-nodes-and-instructions">Backend</a> part for source code details on adding a new backend-specific CPUMaxSplatNode on CPU.</p>
<h3>References</h3>
<ul>
<li><a href="https://arxiv.org/abs/1805.00907">Glow: Graph Lowering Compiler Techniques for Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1802.04799">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</a></li>
<li><a href="https://devblogs.nvidia.com/tensorrt-3-faster-tensorflow-inference/">TensorRT 3: Faster TensorFlow Inference and Volta Support</a></li>
<li><a href="https://github.com/pytorch/glow/issues/1549#issuecomment-416283664">Discussions in Glow issue 1549</a> </li>
</ul>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
