<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Glow: Quantization in Glow</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Glow
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Quantization in Glow </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Introduction</h2>
<p>Quantization is the process of constraining an input from a continuous or otherwise large set of values (such as the real numbers) to a discrete set (such as the integers). In this context, Quantization is the process of converting the inference phase of the neural network execution from floating point arithmetic to integer arithmetic. Arithmetic using small integers is more efficient than the computation of full-width floating-point numbers, and additionally decreases memory usage.</p>
<p>This is an external <a href="https://www.tensorflow.org/performance/quantization">link</a> that explains how quantization is done in TensorFlow.</p>
<p>Glow is able to convert floating-point-based networks into signed 8-bit integer networks. The canonical quantization representation is using signed integers, though it is possible to support other quantization formats. Glow uses profile-guided quantization, observing execution during inference to estimate the possible numeric range for each stage of the neural network. Training-based quantization is considered future work.</p>
<h2>Tensor Representation</h2>
<p>In Glow, tensors are typed and can represent floats, quantized non-floating-point values such as currently supported Int8 (8-bit signed integers), and index types. A quantized tensor's type is made up of the underlying element type (Int8), as well as the possible range of the values in the tensor using 'scale' and 'offset' fields. To convert from the 8-bit integer range of [-128..127] to the floating-point number that they represent, Glow uses the following conversion formula:</p>
<div class="fragment"><div class="line">value = (input - offset) * scale</div></div><!-- fragment --><p>Activations, weights, and variables all use the same type-system and represent information in a uniform way.</p>
<h2>Network Conversion</h2>
<p>Different parts of the network contain floating-point values in different ranges. In some parts, the typical range of the numbers is between zero and one, while in other parts of the network the possible range is in the hundreds. Choosing a single conversion scale for the whole network would not work, because a single scale value could be imprecise for small values and truncate large values.</p>
<p>We use profile-guided information to estimate the possible numeric range for each stage of the neural network. Our quantization conversion works using a two-phase process. First, we statically instrument the network with special profiling nodes that record the ranges of activations that flow in the network, optimize the network including these profiling nodes, and then run inference. Then, we recompile the network using this profile information to convert the network into a quantized form, allowing for static optimization of the quantized graph. We convert portions of the network into islands of integer computation and aim to generate outputs in the range that the original floating-point network produces. During the conversion, for the following types of quantized nodes, we ignore the output's quantization params (if they are provided) and force the output have the same quantization params as the input for performance purpose: </p><div class="fragment"><div class="line">LocalResponseNormalizationNode</div><div class="line">SliceNode</div><div class="line">ReshapeNode</div><div class="line">TopKNode</div><div class="line">GatherNode</div><div class="line">MaxPoolNode</div></div><!-- fragment --><p>The figure below shows a quantized subgraph from Resnet50.</p>
<div class="image">
<img src="resnet50_quantized_subgraph.png" alt="resnet50_quantized_subgraph.png"/>
</div>
<h3>How to perform NN conversion</h3>
<p>The Glow loader tool provides options to execute both profiling and conversion of a NN graph.</p>
<div class="fragment"><div class="line">{dump-profile=profile.yaml```}</div><div class="line">into the ```profile.yaml``` file.</div><div class="line">This information can be used in the process of quantized conversion.</div><div class="line">For example, you can run the following command to capture profile for Resnet50.</div></div><!-- fragment --><p> ./bin/image-classifier tests/images/imagenet/*.png -image-mode=0to1 -m=resnet50 -model-input-name=gpu_0/data -dump-profile="profile.yaml" </p><div class="fragment"><div class="line">By default, everything will be lowered for profiling. This allows for the</div><div class="line">lowered components of nodes to be profiled, allowing for good precision of</div><div class="line">complex nodes. For example, the `SigmoidCrossEntropyWithLogitsNode` has many</div><div class="line">internal nodes that it is lowered to. Without profiling the internal nodes,</div><div class="line">there would be no information on how best to quantize its internal nodes that it</div><div class="line">is lowered to.</div><div class="line"></div><div class="line">Lowering all nodes may cause performance issues for some models, e.g. if a model</div><div class="line">has group Convolutions which explode the size of the graph when lowered, leading</div><div class="line">to long compilation and run time during profiling. Thus, we allow for disabling</div><div class="line">certain NodeKinds for profiling. This means that during quantization, these</div><div class="line">nodes should also not be lowered by the backend. This can be done using the</div><div class="line">command line option `-do-not-lower-nodes-for-profiling` (note: multiple Nodes</div><div class="line">can be listed via comma separation). For example:</div><div class="line"></div><div class="line">```./bin/image-classifier tests/images/imagenet/*.png -image-mode=0to1 -m=shufflenet -model-input-name=gpu_0/data -dump-profile=&quot;shufflenet.yaml&quot; -do-not-lower-nodes-for-profiling=Convolutio</div></div><!-- fragment --><p>The loader supports the following modes (or schemas) of quantization:</p>
<ul>
<li><code>asymmetric</code> - maps the floating data to quantized ranges not necessarily centered on 0. This is the default quantization schema.</li>
<li><code>symmetric</code> - maps the floating data to ranges centered on 0. In practice, this means the symmetric schema may extend the range it needs to capture to make sure 0.0 is at the center of that range. Therefore, this schema potentially wastes some encoding space to enforce the symmetric property, but it comes with the property that the offset is always equal to zero.</li>
<li><code>symmetric with uint8</code> - produces ranges where the offset is always equal to zero but allows the quantized ranges to be either int8 [-128; 127] or uint8 [0; 255]. In practice, this schema represents uint8 ranges using int8 ranges with an offset of -128. Therefore, when using this schema, the produced profile will have two kinds of ranges: one with an offset of 0 and the other with an offset of -128.</li>
<li><code>symmetric with power of 2 scale</code> - produces quantized ranges centered on 0 (symmetric) but also restricts the scale parameter to be a power of 2. Restricting the scale parameter to be a power of 2 might result in a poor exploitation of the quantized range (poor accuracy) but has the potential to provide a better performance.</li>
</ul>
<p>Use <code>quantization-schema=&lt;schema&gt;</code> to specify the schema for the quantization process, where schema will have one of the values:</p>
<ul>
<li><code>asymmetric</code></li>
<li><code>symmetric</code></li>
<li><code>symmetric_with_uint8</code></li>
<li><code>symmetric_with_power2_scale</code></li>
</ul>
<div class="fragment"><div class="line">{load-profile=profile.yaml```}</div><div class="line">captured profile in ```profile.yaml``` file. Important note, graph structure</div><div class="line">should not be changed between a step of capturing profile and a step of quantizing</div><div class="line">the graph.</div><div class="line">For example, you can run the following command to load the profile and quantize</div><div class="line">the graph.</div></div><!-- fragment --><p> ./bin/image-classifier tests/images/imagenet/*.png -image-mode=0to1 -m=resnet50 -model-input-name=gpu_0/data -load-profile="profile.yaml" </p><div class="fragment"><div class="line">By default, all nodes that can be quantized will be quantized. However, we may</div><div class="line">only want to quantize some parts of a model, e.g. if accuracy loss is too high</div><div class="line">when all node kinds are quantized. The Glow loader currently allows for</div><div class="line">disabling quantization of all nodes of a specific kind which are found in the</div><div class="line">graph. For example, if the loaded model sees high accuracy loss when</div><div class="line">element-wise Add is quantized, it can be left in floating point. This can be</div><div class="line">done by passing on the command line the node name via the option</div><div class="line">`-keep-original-precision-for-nodes`. Multiple node kinds can be specified to</div><div class="line">not be quantized. For example, to not quantize any Add or Div nodes when running</div><div class="line">the quantized text translator:</div><div class="line"></div><div class="line">```./bin/text-translator -m en2gr -load-profile=en2gr.yaml -keep-original-precision-for-nodes=Add,Di</div></div><!-- fragment --><p>By default, target quantization precision is int8. However, precision can be controlled via command line parameter: <code>quantization-precision</code>. There are two supported values: <code>Int8</code> and <code>Int16</code>.</p>
<h2>Caffe2 Quantized <a class="el" href="struct_model.html">Model</a> Support</h2>
<p>Glow is able to support Caffe2 Resnet50 quantized model: <a href="https://github.com/caffe2/models/tree/master/resnet50_quantized">https://github.com/caffe2/models/tree/master/resnet50_quantized</a></p>
<p>To support Caffe2 quantized models, Glow has:</p><ul>
<li>Supported additional quantized Caffe2 operators. <div class="fragment"><div class="line">Int8Quantize</div><div class="line">Int8Dequantize</div><div class="line">Int8Conv</div><div class="line">Int8ConvRelu</div><div class="line">Int8MaxPool</div><div class="line">Int8AveragePool</div><div class="line">Int8FC</div><div class="line">Int8SumRelu</div><div class="line">Int8GivenIntTensorFill</div><div class="line">Int8GivenTensorFill</div></div><!-- fragment --></li>
<li>Supported int32 quantized bias.</li>
</ul>
<p>In most of the cases, bias is quantized in int32 to improve precision (the partial sum of the matrix-matrix multiplication is accumulated into int32, so int32 bias can be added to the int32 partial sum for better accuracy). Glow now supports int32 quantized bias in <code>Convolution</code>, <code>FullyConnected</code> and <code>RowwiseQuantizedFullyConnected</code> nodes.</p>
<ul>
<li>Supported the conversion from uint8 quantized activations to int8 quantized activations.</li>
</ul>
<p>For the quantized Caffe2 ops, the activations are quantized to uint8. In Glow, the activations are quantized to int_8. Therefore, for the offset read from quantized Caffe2 model, we need to subtract 128(i.e. INT8_MIN) to make the activations become int8.</p>
<h2>Compiler Optimizations</h2>
<p>Glow features a number of compiler optimizations that transform the compute graph and make it more efficient. There are a few classes of optimizations and parameters to optimize.</p>
<p>First, we attempt to minimize the number of conversions between floating-point tensors and integer tensors, in both directions. Some operations, such as 'transpose' and 'concat' operate on both types, and changing the representation can minimize conversions.</p>
<p>Second, the neural network contains 'rescale' nodes that change the range of the integers. These nodes are required to convert between numeric ranges that mimic the original floating-point network. However, in many cases, it is possible to fold the rescale operations into numeric-producing operations, and eliminate them.</p>
<p>Third, it's possible to rescale the values in the network in order to allow fast hardware implementations of the quantized operations. For example, consider the 'max' operations. By converting both sides of the 'max' into the same scale we allow the hardware to perform a simple comparison. By normalizing both sides of the 'max' operation to the same scale we enable this efficient optimization.</p>
<p>For more specific graph optimizations check <a href="Optimizations.md#quantization-specific-optimizations">here</a>.</p>
<h2>Row-wise Quantization</h2>
<p>Row-wise (or channel-wise) quantization is an important way to minimize accuracy drop. Glow supports row-wise quantized FullyConnected and SparseLengthsWeightedSum nodes; They are enabled by the <a href="Testing.md#model-loader">model loader</a> option "-enable-rowwise".</p>
<p>For the regular quantized FC, we quantize the whole weights tensor with the same scale and offset, which are computed based on the max and min of the entire tensor. But for row-wise, after getting <code>min_i</code> and <code>max_i</code> for each row <code>i</code>, we compute the pair of <code>(scale_i, offset_i)</code> to quantize each element in row <code>i</code>. The figure below shows the quantized FC node and RowwiseQuantizedFullyConnected node. Instead of using only one tensor to represent the quantized weights, we need 2 extra vectors <code>Scales</code> and </p><div class="fragment"><div class="line">{Offsets```}</div><div class="line"></div><div class="line"></div><div class="line">![](rowwise_quantized_fc.png)</div><div class="line"></div><div class="line">Row-wise quantized SparseLengthsWeightedSum is also supported. Similar to the</div><div class="line">above, we compute scales and offsets per row, to be used with the `Data` input</div><div class="line">for the `RowwiseQuantizedSparseLengthsSumNode`. Scales and Offsets are inputs to</div><div class="line">the node. Output of this node is float, matching the Caffe2 implementation.</div><div class="line"></div><div class="line">### Fused Row-wise Quantization</div><div class="line"></div><div class="line">For some backends it may be beneficial to keep each row&#39;s scales and offsets</div><div class="line">fused inline with the data. Caffe2 implements nodes with fused storage, such as</div><div class="line">[SparseLengthsWeightedSum](https://caffe2.ai/docs/operators-catalogue.html#sparselengthsweightedsumfused8bitrowwise). Glow</div><div class="line">supports such fused Nodes/Instructions, for example</div><div class="line">`FusedRowwiseQuantizedSparseLengthsWeightedSum`. The `ElemKind` of fused tensors</div><div class="line">is either `UInt8FusedQTy` or `UInt8FusedFP16QTy`. Tensors with these `ElemKind`s</div><div class="line">are 2-dimensional, and have extra columns for each row to store scales and</div><div class="line">offsets for that row. `UInt8FusedQTy` stores scales and offsets as float (so</div><div class="line">there are 8 extra columns), while `UInt8FusedFP16QTy` stores them as float16_t</div><div class="line">(so there are 4 extra columns). Note that similar to normal row-wise quantized</div><div class="line">tensors, they use a dummy scale and offset in the Type.</div><div class="line"></div><div class="line">### Conversion formula when using row-wise quantization</div><div class="line"></div><div class="line">Some row-wise quantized operators prefer to use float offsets instead of</div><div class="line">int32. For these operators, they use the following conversion formula:</div></div><!-- fragment --><p> value = (scale * input) + offset </p><div class="fragment"><div class="line">Operators using `UInt8FusedQTy` always use float offsets and this alternate</div><div class="line">conversion formula. Nodes that use float offsets and this alternate conversion</div><div class="line">formula are:</div></div><!-- fragment --><p> RowwiseQuantizedSparseLengthsWeightedSum FusedRowwiseQuantizedSparseLengthsWeightedSum ``` </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
