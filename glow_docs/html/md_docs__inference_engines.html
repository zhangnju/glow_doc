<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Glow: Inference Engines</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Glow
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Inference Engines </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><ul>
<li><a href="#inference-engines">Inference Engines</a><ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#x-inference-engine">X-Inference-Engine</a><ul>
<li><a href="#platform-support">Platform Support</a></li>
<li><a href="#building-and-build-options">Building and Build Options</a></li>
<li><a href="#command-line-options">Command Line Options</a><ul>
<li><a href="#notes-on-input-and-output">Notes on Input and Output</a></li>
</ul>
</li>
<li><a href="#examples">Examples</a><ul>
<li><a href="#sample-input">Sample Input</a></li>
<li><a href="#running-with-statically-linked-bundles">Running with Statically-Linked Bundles</a></li>
<li><a href="#running-with-dynamically-loadable-bundles">Running with Dynamically-Loadable Bundles</a></li>
<li><a href="#performance-monitoring-linux-only">Performance Monitoring (Linux Only)</a></li>
</ul>
</li>
<li><a href="#technical-notes-and-notes-on-dependencies">Technical Notes and Notes on Dependencies</a></li>
<li><a href="#a-note-on-the-initial-contribution">A Note on the Initial Contribution</a> #</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>Introduction</h2>
<p>Inference engines are supplied with Glow but are not part of the Glow build system. The engines are included in the separate directory <code>[GLOW ROOT DIR]/inference_engines</code> and must be built separately. CMake files are provided to facilitate building of the engines, and the engines can be cross-compiled for any supported platform. These engines can either be used as stand-alone applications, as part of a backend library powering custom engines, or as examples guiding the development of custom engines. Some engines, or some parts and features of some engines, are only supported on specific platforms (e.g. UNIX or Linux). This is made explicit where applicable in the present documentation. While this is a limitation, one could extend the provided engines to presently unsupported platforms using the current engines as a base. As always, contributions are encouraged and greatly appreciated!</p>
<h2>X-Inference-Engine</h2>
<p>The <code>x-inference-engine</code> was conceived and developed as a complement to the <code>XModelRunner</code> &ndash; the latter being a generic model compiler/runner distributed with Glow along with <code>ImageClassifier</code> and the generic <code>ModelLoader</code>. That is, the <code>x-inference-engine</code> is a generic (for the most part) engine that is able to execute inference using bundles produced by Glow (e.g. the <code>XModelRunner</code> or <code>ImageClassifier</code>, or other model builders). It provides flexibility in terms of bundle handling: bundles can either be supplied at compile time and statically compiled into the engine, or the engine can be compiled with dynamic bundle linking support, in which case bundles (that are built as dynamically linked objects) can be supplied to the engine at run time and loaded as dynamically linked libraries. This engine comes with a backend library as well as a command line application. That is, the engine can either be used as a backend to develop other engines, or it can be used as a stand-alone application.</p>
<h3>Platform Support</h3>
<p>The base functionality of the <code>x-inference-engine</code> is currently supported on all UNIX-like platforms, including OSX and Linux, but is not supported on Windows (its support for Windows is limited to UNIX/Linux virtual machines or simulators such as Cygwin or the Linux Subsystem for Windows, as well as any virtual machines that can host a UNIX-like guest OS). It can also be built and run in any UNIX-like virtual machine (it has been tested extensively with Linux running natively, or as a guest provided by VirtualBox). Finally, some functionality is currently limited to Linux only (e.g. performance monitoring). During the development of the engine, the decision to include this functionality was not clear; the main reason it was included is to (1) provide some rudimentary performance monitoring support (albeit only in Linux for now), and (2) serve the currently supported Linux-only feature as an example to guide further development of this functionality and its extension to other platforms.</p>
<h3>Building and Build Options</h3>
<p>Before you begin, please make sure to read the notes on dependencies in <a href="#technical-notes-and-notes-on-dependencies">Notes on Dependencies</a>.</p>
<p>There is a <code>CMakeLists.txt</code> supplied with the engine. To build the engine in the <code>build</code> directory under, for example, <code>[GLOW ROOT]/inference_eingines/build</code>, execute the command </p><div class="fragment"><div class="line">cmake -G Ninja -DCMAKE_BUILD_TYPE=[Debug/Release] [ADDITIONAL OPTIONS] ../</div></div><!-- fragment --><p> followed by </p><div class="fragment"><div class="line">ninja all</div></div><!-- fragment --><p>Of course, <code>Ninja</code> can be substituted with any other supported build engine, such as <code>Unix Makefiles</code>. The additional options, some required and some optional, are given in the table below. Note also that cross-compilation can be facilitated by the <code>TARGET_OPTIONS</code> and <code>TARGET_LINKER_FLAGS</code> options (see below for details). Of course, in this case, when compiling with statically linked bundles, one must make sure that the bundles were also cross-compiled for the target platform.</p>
<table class="doxtable">
<tr>
<th align="left">Option </th><th align="left">Expected Values </th><th align="left">Description </th><th align="left">Required </th><th align="left">Notes  </th></tr>
<tr>
<td align="left"><code>LIB_ONLY</code> </td><td align="left">ON/OFF </td><td align="left">Whether to build the backend library only (without command line application) </td><td align="left">No (default is OFF) </td><td align="left"></td></tr>
<tr>
<td align="left"><code>BUILD_FOR_DYNAMIC_LINKAGE</code> </td><td align="left">ON/OFF </td><td align="left">Whether to build for dynamic linkage (bundle not specified at compile time, but must be specified at runtime and linked in dynamically &ndash; bundle must be dynamically linkable) </td><td align="left">No (default is ON) </td><td align="left"></td></tr>
<tr>
<td align="left"><code>LINK_LIBS_STATICALLY</code> </td><td align="left">ON/OFF </td><td align="left">Whether to link everything (e.g. <code>glibc</code>, <code>libm</code>, etc.) statically </td><td align="left">No (default is ON) </td><td align="left">Static linkage generally inflates object footprint, but may be required when compiling for platforms that may exhibit library version mismatch or might not even carry the required libraries </td></tr>
<tr>
<td align="left"><code>ENABLE_PERF_MONITORING</code> </td><td align="left">ON/OFF </td><td align="left">Whether to enable performance monitoring </td><td align="left">No (default is OFF) </td><td align="left">Supported on Linux only </td></tr>
<tr>
<td align="left"><code>TARGET_OPTIONS</code> </td><td align="left">A string of additional compile options </td><td align="left">Provides the ability to specify additional compiler options (e.g. include directories, sysroot, etc &ndash; may be helpful when cross-compiling) </td><td align="left">No </td><td align="left"></td></tr>
<tr>
<td align="left"><code>TARGET_LINKER_FLAGS</code> </td><td align="left">A string of additional linker options </td><td align="left">Provides the ability to specify additional linker options (e.g. libraries, sysroot, etc &ndash; may be helpful when cross-compiling) </td><td align="left">No </td><td align="left"></td></tr>
<tr>
<td align="left"><code>LINKED_BUNDLE</code> </td><td align="left">The bundle path </td><td align="left">Specifies the bundle to be compiled in when not compiling for dynamic linkage </td><td align="left">Yes only when <code>BUILD_FOR_DYNAMIC_LINKAGE</code> is OFF </td><td align="left"></td></tr>
<tr>
<td align="left"><code>LINKED_MODEL_NAME</code> </td><td align="left">The model name </td><td align="left">Specifies the model name (the base name for symbols in the specified bundle) </td><td align="left">Yes only when <code>BUILD_FOR_DYNAMIC_LINKAGE</code> is OFF </td><td align="left"></td></tr>
</table>
<p>The produced output is the static library <code>xinfer</code>, and, if <code>LIB_ONLY</code> is set to OFF, the stand-alone executable <code>x-infer</code>.</p>
<h3>Command Line Options</h3>
<p>The stand-alone application accepts many command line options, most of which are required. These are described in the table below, and the documentation can be written to <code>stdout</code> with the <code>--help</code> option. The options may differ depending on whether the application was compiled with a statically-linked bundle, or with support for dynamic bundle linkage.</p>
<table class="doxtable">
<tr>
<th align="left">Option </th><th align="left">Expected Values </th><th align="left">Description </th><th align="left">Required </th><th align="left">Notes  </th></tr>
<tr>
<td align="left"><code>output</code> </td><td align="left">File name </td><td align="left">Output to the specified file instead of <code>stdout</code> </td><td align="left">No </td><td align="left">If omitted, output is written to <code>stdout</code> </td></tr>
<tr>
<td align="left"><code>infile</code> </td><td align="left">File name </td><td align="left">File containing input(s) in binary format </td><td align="left">Yes </td><td align="left"></td></tr>
<tr>
<td align="left"><code>intype</code> </td><td align="left"><code>F32</code>, <code>F16</code>, <code>I16</code>, or <code>I8</code> </td><td align="left">Input data type </td><td align="left">Yes </td><td align="left"></td></tr>
<tr>
<td align="left"><code>outtype</code> </td><td align="left"><code>F32</code>, <code>F16</code>, <code>I16</code>, or <code>I8</code> </td><td align="left">Output data type </td><td align="left">Yes </td><td align="left"></td></tr>
<tr>
<td align="left"><code>inlen</code> </td><td align="left">Integer </td><td align="left">Input tensor length (e.g. if the tensor is of shape <code>2x3x4</code>, its length is <code>2 * 3 * 4 = 24</code> </td><td align="left">Yes </td><td align="left"></td></tr>
<tr>
<td align="left"><code>outlen</code> </td><td align="left">Integer </td><td align="left">Output tensor length (e.g. if the tensor is of shape <code>2x3x4</code>, its length is <code>2 * 3 * 4 = 24</code> </td><td align="left">Yes </td><td align="left">Only single output tensor is currently supported </td></tr>
<tr>
<td align="left"><code>inname</code> </td><td align="left">String (no spaces) </td><td align="left">Input tensor name </td><td align="left">Yes </td><td align="left">Only single named input tensors are currently supported </td></tr>
<tr>
<td align="left"><code>outname</code> </td><td align="left">String (no spaces) </td><td align="left">Output tensor name </td><td align="left">Yes </td><td align="left">Only single named output tensors are currently supported </td></tr>
<tr>
<td align="left"><code>perf</code> </td><td align="left">Optionless Flag </td><td align="left">Whether to output performance logs </td><td align="left">No (default is NO) </td><td align="left">Only supported on Linux, and only available when compiled with the <code>ENABLE_PERF_MONITORING</code> option </td></tr>
<tr>
<td align="left"><code>perflog</code> </td><td align="left">String (no spaces) </td><td align="left">Performance log output filename (<code>stdout</code> if omitted) </td><td align="left">No </td><td align="left">Only supported on Linux, and only available when compiled with the <code>ENABLE_PERF_MONITORING</code> option </td></tr>
<tr>
<td align="left"><code>model</code> </td><td align="left">String (no spaces) </td><td align="left"><a class="el" href="struct_model.html">Model</a> name (maximum 128 characters) </td><td align="left">Yes </td><td align="left">Only available when compiled with the <code>BUILD_FOR_DYNAMIC_LINKAGE</code> option </td></tr>
</table>
<p>The positional arguments are described below.</p>
<table class="doxtable">
<tr>
<th align="left">Argument </th><th align="left">Expected Values </th><th align="left">Description </th><th align="left">Required </th><th align="left">Notes  </th></tr>
<tr>
<td align="left">Weights file </td><td align="left">String </td><td align="left">File name of the weights file </td><td align="left">Yes </td><td align="left">Specified as the first positional argument if the engine was compiled with a statically linked bundle; otherwise, this is the second positional argument </td></tr>
<tr>
<td align="left">Bundle file </td><td align="left">String </td><td align="left">File name of the dynamically linked bundle </td><td align="left">Yes </td><td align="left">Specified as the first positional argument if the engine was compiled with support for dynamic bundle linkage; otherwise, not applicable </td></tr>
</table>
<h4>Notes on Input and Output</h4>
<p>Input(s) are given in a single binary file. The layout is a bytestream. The application is able to break the file up into individual inputs knowing the single input length (specified with the <code>inlen</code> and <code>intype</code>) option. For instance, if the input consists of a single floating 32bit point number, then the input file containing two inputs would consist of 8 bytes, the first four containing the first input, and the last four containing the last input.</p>
<p>The output is composed in a similar way as a bytestream saved into a single binary file. The order of output bytes follows the order of input bytes in the input file.</p>
<p>It is a responsibility of the developer to take care of issues such as byte ordering (e.g. big- vs little-endianness) in both, the input/output pairs and the weights file.</p>
<h3>Examples</h3>
<p>For the examples, we shall use the following fully connected feed forward network that has been trained to identify the sine curve on the interval <code>[0, 2pi]</code>. Everything above the curve is assigned the class <code>0</code>, while everything below the curve is assigned the class <code>1</code>.</p>
<div class="fragment"><div class="line">import numpy as np</div><div class="line">import torch</div><div class="line">import random</div><div class="line"></div><div class="line">class FeedForwardNN(torch.nn.Module):</div><div class="line">    def __init__(self, nx, nh, ny, depth, hidden_activation = torch.nn.ReLU(),</div><div class="line">                 output_activation = torch.nn.Sigmoid(), seed = None, dtype = torch.FloatTensor):</div><div class="line">        super().__init__()</div><div class="line"></div><div class="line">        if seed is not None:</div><div class="line">            torch.manual_seed(seed)</div><div class="line"></div><div class="line">        self.params = torch.nn.ParameterList()</div><div class="line"></div><div class="line">        self.layers = [torch.nn.Linear(nx, nh).type(dtype)]</div><div class="line">        self.layers.append(hidden_activation)</div><div class="line"></div><div class="line">        for _ in range(depth):</div><div class="line">            self.layers.append(torch.nn.Linear(nh, nh).type(dtype))</div><div class="line">            self.layers.append(hidden_activation)</div><div class="line"></div><div class="line">        self.layers.append(torch.nn.Linear(nh, ny).type(dtype))</div><div class="line">        self.layers.append(output_activation)</div><div class="line"></div><div class="line">        for layer in self.layers:</div><div class="line">            self.params.extend(layer.parameters())</div><div class="line"></div><div class="line">    def forward(self, x):</div><div class="line">        y = x</div><div class="line">        for layer in self.layers:</div><div class="line">            y = layer(y)</div><div class="line"></div><div class="line">        return y</div><div class="line"></div><div class="line">def train(module, loss, X, Y, num_iters, learning_rate = 1e-1):</div><div class="line">    num_samples = X.shape[0]</div><div class="line">    optimizer = torch.optim.Adam(module.parameters(), lr=learning_rate)</div><div class="line"></div><div class="line">    for _ in range(num_iters):</div><div class="line">        optimizer.zero_grad()</div><div class="line">        cost = loss(module(X), Y).sum() / num_samples </div><div class="line">        cost.backward()</div><div class="line">        optimizer.step()</div><div class="line"></div><div class="line"># Number of training and test samples.</div><div class="line">m_train = 10000</div><div class="line">m_test  = 5000</div><div class="line"></div><div class="line"># Generate our data - points in 2D separated by a sine curve.</div><div class="line">#</div><div class="line"># We set the seed to make sure the experiment is reproduceable.</div><div class="line">np.random.seed(1)</div><div class="line"></div><div class="line">x_train = np.random.rand(m_train, 2) * 2 - 1</div><div class="line">y_train = np.array([[0 if p[1] &lt; 0.5 * np.sin(2 * np.pi * p[0]) else 1 for p in x_train]])</div><div class="line"></div><div class="line"># Intentionally misclassify 5% of the points.</div><div class="line">m_train_missed = m_train // 20</div><div class="line">indices = random.sample(range(0, m_train), m_train_missed)</div><div class="line">for index in indices:</div><div class="line">    y_train[0, index] = 0 if y_train[0, index] != 0 else 1</div><div class="line"></div><div class="line"># Set the structure of our network...</div><div class="line">nx    = 2    # Dimension of input vectors.</div><div class="line">nh    = 8    # Number of neurons in each hidden layer.</div><div class="line">ny    = 1    # Dimension of the output vector.</div><div class="line">depth = 2    # Number of hidden layers.</div><div class="line">seed  = 0    # The seed for random initialization of the weights.</div><div class="line"></div><div class="line"># Logistic regression loss function for computing the cost.</div><div class="line">def logit_loss(yhat, y):</div><div class="line">    return -(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))</div><div class="line"></div><div class="line">ffnn = FeedForwardNN(nx, nh, ny, depth, seed=seed)</div><div class="line"></div><div class="line"># Convert numpy data into torch tensors...</div><div class="line">torch_train_input = torch.from_numpy(x_train).float()</div><div class="line">torch_train_output = torch.from_numpy(y_train.T).float()</div><div class="line"></div><div class="line"># Train the network...</div><div class="line">train(ffnn, logit_loss, torch_train_input, torch_train_output, 2000)</div><div class="line"></div><div class="line"># Set training mode to False, since at this point we&#39;re only interested</div><div class="line"># in inference.</div><div class="line">ffnn.train(False)</div><div class="line"></div><div class="line"># Create dummy input to go along with our model.</div><div class="line">inp = torch.autograd.Variable(torch_test_input[:1])</div><div class="line"></div><div class="line">input_names = [&quot;in&quot;]</div><div class="line">output_names = [&quot;out&quot;]</div><div class="line"></div><div class="line">torch.onnx.export(ffnn, inp, &quot;ffnn.onnx&quot;, input_names=input_names,</div><div class="line">                  output_names=output_names, verbose=True, export_params=True)</div></div><!-- fragment --><p>The sample output from ONNX is below.</p>
<div class="fragment"><div class="line">graph(%in : Float(1, 2)</div><div class="line">%1 : Float(8, 2)</div><div class="line">%2 : Float(8)</div><div class="line">%3 : Float(8, 8)</div><div class="line">%4 : Float(8)</div><div class="line">%5 : Float(8, 8)</div><div class="line">%6 : Float(8)</div><div class="line">%7 : Float(1, 8)</div><div class="line">%8 : Float(1)) {</div><div class="line">%9 : Float(1, 8) = onnx::Gemm[alpha=1, beta=1, transB=1](%in, %1, %2), scope: FeedForwardNN/Linear</div><div class="line">%10 : Float(1, 8) = onnx::Relu(%9), scope: FeedForwardNN/ReLU</div><div class="line">%11 : Float(1, 8) = onnx::Gemm[alpha=1, beta=1, transB=1](%10, %3, %4), scope: FeedForwardNN/ReLU</div><div class="line">%12 : Float(1, 8) = onnx::Relu(%11), scope: FeedForwardNN/ReLU</div><div class="line">%13 : Float(1, 8) = onnx::Gemm[alpha=1, beta=1, transB=1](%12, %5, %6), scope: FeedForwardNN/ReLU</div><div class="line">%14 : Float(1, 8) = onnx::Relu(%13), scope: FeedForwardNN/ReLU</div><div class="line">%15 : Float(1, 1) = onnx::Gemm[alpha=1, beta=1, transB=1](%14, %7, %8), scope: FeedForwardNN/ReLU</div><div class="line">%out : Float(1, 1) = onnx::Sigmoid(%15), scope: FeedForwardNN/Sigmoid</div><div class="line">return (%out);</div><div class="line">}</div></div><!-- fragment --><p>What is important to note here is that the input tensor name is <code>in</code> and the output tensor name is <code>out</code>. Also note that the input tensor dimension is <code>&lt;1, 2&gt;</code>, and the output tensor dimension is <code>&lt;1, 1&gt;</code>. The output tensor carries the probability of belonging to class <code>0</code>. This information will be used later.</p>
<p>Now let us produce two versions of bundles: one for static linking, and one suitable for dynamic linking. Make sure to change all the relative paths below to reflect your setup (we assume that you have built Glow and the included builders, in particular the <code>x-model-builder</code>)</p>
<p><b>Statically linkable bundle:</b> </p><div class="fragment"><div class="line">(base) wyessen [~/dts/nn/xgmr/build/debug/osx] $ ./bin/x-model-builder -input-tensor-dims=1,2 -output-tensor-names=out -model-input-name=in -model=../../../../pytorch_tuts/ffnn.onnx -emit-bundle=../output/ -backend=CPU -main-entry-name=ffnn -network-name=ffnn_static</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/build/debug/osx] $ ls ../output/</div><div class="line">ffnn_static.o       ffnn_static.weights</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/build/debug/osx] $ </div></div><!-- fragment --><p><b>Dynamically linkable bundle:</b> </p><div class="fragment"><div class="line">(base) wyessen [~/dts/nn/xgmr/build/debug/osx] $ ./bin/x-model-builder -input-tensor-dims=1,2 -output-tensor-names=out -model-input-name=in -model=../../../../pytorch_tuts/ffnn.onnx -emit-bundle=../output/ -backend=CPU -main-entry-name=ffnn -network-name=ffnn_dynamic -llvm-compiler=/usr/local/opt/llvm@8/bin/clang++ -llvm-compiler-opt=-shared</div><div class="line"></div><div class="line">warning: overriding the module target triple with x86_64-apple-macosx10.14.0 [-Woverride-module]</div><div class="line">1 warning generated.</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/build/debug/osx] $ ls ../output/</div><div class="line">ffnn_dynamic.bc      ffnn_dynamic.o       ffnn_dynamic.weights ffnn_static.o        ffnn_static.weights</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/build/debug/osx] $ </div></div><!-- fragment --><p>You can safely ignore the generated warning if you are compiling on OSX.</p>
<h4>Sample Input</h4>
<p>Let us generate sample input for our inference engine. We'll provide two inputs (four floats in a single binary file, two per input, since input dimension is <code>&lt;1, 2&gt;</code>). Generate it like this: </p><div class="fragment"><div class="line">(base) wyessen [~/dts/nn/xgmr/build/debug/output] $ perl -e &#39;print pack(&quot;f*&quot;, 0.123, 0.321, 0.456, 0.654)&#39; &gt; sample_input.dat</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/build/debug/output] $ od -f sample_input.dat </div><div class="line"></div><div class="line">0000000     1.230000e-01    3.210000e-01    4.560000e-01    6.540000e-01</div><div class="line">0000020</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/build/debug/output] $ </div></div><!-- fragment --><h4>Running with Statically-Linked Bundles</h4>
<p>Now with the bundles produced, let us go ahead and compile the inference engine. In this case, we are going to specify the bundle at compile time, statically linking it in. We won't statically link all the other required libraries (e.g. the math library or the GNU argp library).</p>
<p>The command line to prepare the build system is as follows (sample output is also included). </p><div class="fragment"><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ cmake -G Ninja -DLIB_ONLY=OFF -DBUILD_FOR_DYNAMIC_LINKAGE=OFF -DLINK_LIBS_STATICALLY=OFF -DENABLE_PERF_MONITORING=OFF -DLINKED_BUNDLE=../../../../../build/debug/output/ffnn_static.o -DLINKED_MODEL_NAME=ffnn ../../../ -DTARGET_OPTIONS=-largp</div><div class="line"></div><div class="line">-- The C compiler identification is AppleClang 10.0.1.10010046</div><div class="line">-- The CXX compiler identification is AppleClang 10.0.1.10010046</div><div class="line">-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc</div><div class="line">-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc -- works</div><div class="line">-- Detecting C compiler ABI info</div><div class="line">-- Detecting C compiler ABI info - done</div><div class="line">-- Detecting C compile features</div><div class="line">-- Detecting C compile features - done</div><div class="line">-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++</div><div class="line">-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ -- works</div><div class="line">-- Detecting CXX compiler ABI info</div><div class="line">-- Detecting CXX compiler ABI info - done</div><div class="line">-- Detecting CXX compile features</div><div class="line">-- Detecting CXX compile features - done</div><div class="line">Will build the library and the executable</div><div class="line">-- Configuring done</div><div class="line">-- Generating done</div><div class="line">-- Build files have been written to: /Users/wyessen/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ </div></div><!-- fragment --><p>Note that since we're building on OSX, we had to explicitly specify <code>-largp</code> (after installing it with Brew using <code>brew install argp-standalone</code>).</p>
<p>We can now build with <code>ninja all</code>: </p><div class="fragment"><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ ninja all</div><div class="line"></div><div class="line">[1/5] Building C object CMakeFiles/xinfer.dir/x_perf_monitor.c.oclang: warning: -largp: &#39;linker&#39; input unused [-Wunused-command-line-argument]</div><div class="line">[2/5] Building C object CMakeFiles/xinfer.dir/x_inference_lib.c.o</div><div class="line">clang: warning: -largp: &#39;linker&#39; input unused [-Wunused-command-line-argument]</div><div class="line">[3/5] Building C object CMakeFiles/x-infer.dir/main.c.o</div><div class="line">clang: warning: -largp: &#39;linker&#39; input unused [-Wunused-command-line-argument]</div><div class="line">[4/5] Linking C static library bin/libxinfer.a</div><div class="line">/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bin/libxinfer.a(x_perf_monitor.c.o) has no symbols</div><div class="line">/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bin/libxinfer.a(x_perf_monitor.c.o) has no symbols</div><div class="line">[5/5] Linking C executable bin/x-infer</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ </div></div><!-- fragment --><p>You can safely ignore the generated warnings. At this point the inference engine has been built. You can verify this as follows.</p>
<div class="fragment"><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ ./bin/x-infer --help</div><div class="line"></div><div class="line">Usage: x-infer [OPTION...] [WEIGHTS FILENAME]</div><div class="line"></div><div class="line">                    Generic Inference Engine                         </div><div class="line">-----------------------------------------------------------------------</div><div class="line">Dynamic bundle loading: NOT SUPPORTED (bundle has been statically linked)</div><div class="line">Performance monitoring: NOT SUPPORTED</div><div class="line"></div><div class="line">x-infer runs inference against the provided Glow bundle. Weights file must be</div><div class="line">specified as the first argument. The input file must be specified with</div><div class="line">[--infile] (binary file). Input tensor type [-intype], output tensor type</div><div class="line">[-outtype], input length [--inlen], output length [--outlen], input tensor name</div><div class="line">[-inname], and output tensor name [--outname] must be specified. </div><div class="line"></div><div class="line">When built with dynamic bundle loading support, bundle must be specified as the</div><div class="line">first positional argument, and the weights file as the second. When built with</div><div class="line">a bundle statically linked in, dynamic loading is not supported </div><div class="line"></div><div class="line">Short and long form options are: </div><div class="line"></div><div class="line">  -i, --infile=FILE          Input from FILE</div><div class="line">  -l, --inlen=LEN            Input tensor length (e.g. if the tensor is of</div><div class="line">                             shape 2x3x4, its length is 2 * 3 * 4 = 24)</div><div class="line">  -L, --outlen=LEN           Output tensor length (e.g. if the tensor is of</div><div class="line">                             shape 2x3x4, its length is 2 * 3 * 4 = 24)</div><div class="line">  -n, --inname=NAME          Input tensor name NAME</div><div class="line">  -N, --outname=NAME         Output tensor name NAME</div><div class="line">  -o, --output=FILE          Output to binary FILE instead of standard output</div><div class="line">  -t, --intype=TYPE          Input TYPE (one of F32, F16, I16, I8)</div><div class="line">  -T, --outtype=TYPE         Output TYPE (one of F32, F16, I16, I8)</div><div class="line">  -?, --help                 Give this help list</div><div class="line">      --usage                Give a short usage message</div><div class="line">  -V, --version              Print program version</div><div class="line"></div><div class="line">Mandatory or optional arguments to long options are also mandatory or optional</div><div class="line">for any corresponding short options.</div><div class="line"></div><div class="line">Report bugs to Github Pytorch Glow repository at</div><div class="line">https://github.com/pytorch/glow.</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ </div></div><!-- fragment --><p>Now let us run on our sample input, saving the output in <code>output.dat</code>, and then using <code>od</code> to interpret the output. </p><div class="fragment"><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ ./bin/x-infer --infile=../../../../../build/debug/output/sample_input.dat --inlen=2 --outlen=1 --inname=in --outname=save_out --output=./output.dat --intype=F32 --outtype=F32 ../../../../../build/debug/output/ffnn_static.weights </div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ od -f output.dat </div><div class="line"></div><div class="line">0000000     5.850238e-01    9.525478e-01                                </div><div class="line">0000010</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ </div></div><!-- fragment --><p>Note that we used the output name <code>save_out</code> instead of <code>out</code>. The reason for this is because Glow, upon bundle compilation, adds the prefix <code>save_</code> to the output tensor name.</p>
<h4>Running with Dynamically-Loadable Bundles</h4>
<p>First, let us recompile the inference engine with support for dynamic bundle loading: </p><div class="fragment"><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ cmake -G Ninja -DLIB_ONLY=OFF -DBUILD_FOR_DYNAMIC_LINKAGE=ON -DLINK_LIBS_STATICALLY=OFF -DENABLE_PERF_MONITORING=OFF ../../../ -DTARGET_OPTIONS=-largp</div><div class="line"></div><div class="line">-- The C compiler identification is AppleClang 10.0.1.10010046</div><div class="line">-- The CXX compiler identification is AppleClang 10.0.1.10010046</div><div class="line">-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc</div><div class="line">-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc -- works</div><div class="line">-- Detecting C compiler ABI info</div><div class="line">-- Detecting C compiler ABI info - done</div><div class="line">-- Detecting C compile features</div><div class="line">-- Detecting C compile features - done</div><div class="line">-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++</div><div class="line">-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ -- works</div><div class="line">-- Detecting CXX compiler ABI info</div><div class="line">-- Detecting CXX compiler ABI info - done</div><div class="line">-- Detecting CXX compile features</div><div class="line">-- Detecting CXX compile features - done</div><div class="line">Will build the library and the executable</div><div class="line">Will build the executable for dynamic bundle linkage</div><div class="line">-- Configuring done</div><div class="line">-- Generating done</div><div class="line">-- Build files have been written to: /Users/wyessen/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ </div></div><!-- fragment --><p>Followed by</p>
<div class="fragment"><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ ninja all</div><div class="line"></div><div class="line">[1/5] Building C object CMakeFiles/xinfer.dir/x_perf_monitor.c.o</div><div class="line">clang: warning: -largp: &#39;linker&#39; input unused [-Wunused-command-line-argument]</div><div class="line">[2/5] Building C object CMakeFiles/xinfer.dir/x_inference_lib.c.o</div><div class="line">clang: warning: -largp: &#39;linker&#39; input unused [-Wunused-command-line-argument]</div><div class="line">[3/5] Building C object CMakeFiles/x-infer.dir/main.c.o</div><div class="line">clang: warning: -largp: &#39;linker&#39; input unused [-Wunused-command-line-argument]</div><div class="line">[4/5] Linking C static library bin/libxinfer.a</div><div class="line">/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bin/libxinfer.a(x_perf_monitor.c.o) has no symbols</div><div class="line">/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file: bin/libxinfer.a(x_perf_monitor.c.o) has no symbols</div><div class="line">[5/5] Linking C executable bin/x-infer</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ </div></div><!-- fragment --><p>Again, ignore the generated warnings. You can confirm that dynamic linkage is now supported:</p>
<div class="fragment"><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ ./bin/x-infer --help</div><div class="line"></div><div class="line">Usage: x-infer [OPTION...] [BUNDLE FILENAME] [WEIGHTS FILENAME]</div><div class="line"></div><div class="line">                    Generic Inference Engine                         </div><div class="line">-----------------------------------------------------------------------</div><div class="line">Dynamic bundle loading: SUPPORTED</div><div class="line">Performance monitoring: NOT SUPPORTED</div><div class="line"></div><div class="line">x-infer runs inference against the provided Glow bundle. Weights file must be</div><div class="line">specified as the first argument. The input file must be specified with</div><div class="line">[--infile] (binary file). Input tensor type [-intype], output tensor type</div><div class="line">[-outtype], input length [--inlen], output length [--outlen], input tensor name</div><div class="line">[-inname], and output tensor name [--outname] must be specified. </div><div class="line"></div><div class="line">When built with dynamic bundle loading support, bundle must be specified as the</div><div class="line">first positional argument, and the weights file as the second. When built with</div><div class="line">a bundle statically linked in, dynamic loading is not supported </div><div class="line"></div><div class="line">Short and long form options are: </div><div class="line"></div><div class="line">  -i, --infile=FILE          Input from FILE</div><div class="line">  -l, --inlen=LEN            Input tensor length (e.g. if the tensor is of</div><div class="line">                             shape 2x3x4, its length is 2 * 3 * 4 = 24)</div><div class="line">  -L, --outlen=LEN           Output tensor length (e.g. if the tensor is of</div><div class="line">                             shape 2x3x4, its length is 2 * 3 * 4 = 24)</div><div class="line">  -m, --model=NAME           Model name (maximum 128 chars)</div><div class="line">  -n, --inname=NAME          Input tensor name NAME</div><div class="line">  -N, --outname=NAME         Output tensor name NAME</div><div class="line">  -o, --output=FILE          Output to binary FILE instead of standard output</div><div class="line">  -t, --intype=TYPE          Input TYPE (one of F32, F16, I16, I8)</div><div class="line">  -T, --outtype=TYPE         Output TYPE (one of F32, F16, I16, I8)</div><div class="line">  -?, --help                 Give this help list</div><div class="line">      --usage                Give a short usage message</div><div class="line">  -V, --version              Print program version</div><div class="line"></div><div class="line">Mandatory or optional arguments to long options are also mandatory or optional</div><div class="line">for any corresponding short options.</div><div class="line"></div><div class="line">Report bugs to Github Pytorch Glow repository at</div><div class="line">https://github.com/pytorch/glow.</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ </div></div><!-- fragment --><p>We can now run the inference engine, specifying the bundle we wish to use as the first positional argument, as well as the <code>--model</code> option specifying the model name:</p>
<div class="fragment"><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ ./bin/x-infer --infile=../../../../../build/debug/output/sample_input.dat --inlen=2 --outlen=1 --inname=in --outname=save_out --output=./output.dat --intype=F32 --outtype=F32 ../../../../../build/debug/output/ffnn_dynamic.o ../../../../../build/debug/output/ffnn_dynamic.weights --model=ffnn</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ od -f output.dat </div><div class="line">0000000     5.850238e-01    9.525478e-01                                </div><div class="line">0000010</div><div class="line"></div><div class="line">(base) wyessen [~/dts/nn/xgmr/inference_engines/x-inference-engines/build/native/debug] $ </div></div><!-- fragment --><h4>Performance Monitoring (Linux Only)</h4>
<p>We can compile with the <code>-DENABLE_PERF_MONITORING</code> option (Linux only) to enable performance monitoring. After this, performance will be reported as in the following example. Note that not all VMs support this, and when running in a VM you may not get sensible results. Also, when running natively, it is often the case that you'll need to run with <code>sudo</code> to avoid permission errors (due to how performance metrics are gathered, elevated privileges may be required).</p>
<div class="fragment"><div class="line">wyessen@raspberrypi:~/workbench/glow $ sudo ./bin/x-infer ./bundles/00_ffnn_nonquantized/ffnn.o ./bundles/00_ffnn_nonquantized/ffnn.weights -i ./data/00_ffnn_nonquantized/00_dummy_input.dat -l 2 -L 1 -m ffnn -n in -N save_out -o output.dat -p -t F32 -T F32</div><div class="line"></div><div class="line">Constant weights size       : 896 bytes</div><div class="line">Number of cases             : 1</div><div class="line">Number of CPU cycles (x1-e6): 0.014533</div></div><!-- fragment --><p> Note that CPU cycles are counted in millions.</p>
<h3>Technical Notes and Notes on Dependencies</h3>
<ol type="1">
<li>Currently the engine is written in pure C (C99) and as such can be built and linked with pure C tools.</li>
<li>As mentioned above, only UNIX-like platforms are currently supported (and for some features, only Linux). In addition, GNU argument parsing library (GNU Argp) is required when building the stand-alone application. Please consult the appropriate documentation for your platform for help with installation. It is provided by default with the GNU build tools on many UNIX(-like) platforms, including Linux, and can be installed with <code>brew install argp-standalone</code> on OSX. When compiling on OSX, the additional <code>-DTARGET_OPTIONS=-largp</code> must be specified.</li>
<li>Streaming is currently not supported, but this feature is planned.</li>
<li>More than one named input, or more than one named output, is currently not supported. This support is planned in the future.</li>
<li>Error reporting is rather rudimentary. More detailed error messages are needed. This is planned for the future.</li>
<li>It may be possible to figure out input and output tensor lengths from the bundle, so that the <code>inlen</code> and <code>outlen</code> options can be safely dropped. This is planned for the future. Similarly for input and output types (although this would be more challenging).</li>
</ol>
<h3>A Note on the Initial Contribution</h3>
<p>The initial contribution of the <code>x-inference-engine</code> (as well as the corresponding documentation) was made as part of the open source contribution initiative by the XPERI Corporation (xperi.com). </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
