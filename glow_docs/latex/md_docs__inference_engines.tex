
\begin{DoxyItemize}
\item \href{#inference-engines}{\tt Inference Engines}
\begin{DoxyItemize}
\item \href{#introduction}{\tt Introduction}
\item \href{#x-inference-engine}{\tt X-\/\+Inference-\/\+Engine}
\begin{DoxyItemize}
\item \href{#platform-support}{\tt Platform Support}
\item \href{#building-and-build-options}{\tt Building and Build Options}
\item \href{#command-line-options}{\tt Command Line Options}
\begin{DoxyItemize}
\item \href{#notes-on-input-and-output}{\tt Notes on Input and Output}
\end{DoxyItemize}
\item \href{#examples}{\tt Examples}
\begin{DoxyItemize}
\item \href{#sample-input}{\tt Sample Input}
\item \href{#running-with-statically-linked-bundles}{\tt Running with Statically-\/\+Linked Bundles}
\item \href{#running-with-dynamically-loadable-bundles}{\tt Running with Dynamically-\/\+Loadable Bundles}
\item \href{#performance-monitoring-linux-only}{\tt Performance Monitoring (Linux Only)}
\end{DoxyItemize}
\item \href{#technical-notes-and-notes-on-dependencies}{\tt Technical Notes and Notes on Dependencies}
\item \href{#a-note-on-the-initial-contribution}{\tt A Note on the Initial Contribution} \#
\end{DoxyItemize}
\end{DoxyItemize}
\end{DoxyItemize}

\subsection*{Introduction}

Inference engines are supplied with Glow but are not part of the Glow build system. The engines are included in the separate directory {\ttfamily \mbox{[}G\+L\+OW R\+O\+OT D\+IR\mbox{]}/inference\+\_\+engines} and must be built separately. C\+Make files are provided to facilitate building of the engines, and the engines can be cross-\/compiled for any supported platform. These engines can either be used as stand-\/alone applications, as part of a backend library powering custom engines, or as examples guiding the development of custom engines. Some engines, or some parts and features of some engines, are only supported on specific platforms (e.\+g. U\+N\+IX or Linux). This is made explicit where applicable in the present documentation. While this is a limitation, one could extend the provided engines to presently unsupported platforms using the current engines as a base. As always, contributions are encouraged and greatly appreciated!

\subsection*{X-\/\+Inference-\/\+Engine}

The {\ttfamily x-\/inference-\/engine} was conceived and developed as a complement to the {\ttfamily X\+Model\+Runner} -- the latter being a generic model compiler/runner distributed with Glow along with {\ttfamily Image\+Classifier} and the generic {\ttfamily Model\+Loader}. That is, the {\ttfamily x-\/inference-\/engine} is a generic (for the most part) engine that is able to execute inference using bundles produced by Glow (e.\+g. the {\ttfamily X\+Model\+Runner} or {\ttfamily Image\+Classifier}, or other model builders). It provides flexibility in terms of bundle handling\+: bundles can either be supplied at compile time and statically compiled into the engine, or the engine can be compiled with dynamic bundle linking support, in which case bundles (that are built as dynamically linked objects) can be supplied to the engine at run time and loaded as dynamically linked libraries. This engine comes with a backend library as well as a command line application. That is, the engine can either be used as a backend to develop other engines, or it can be used as a stand-\/alone application.

\subsubsection*{Platform Support}

The base functionality of the {\ttfamily x-\/inference-\/engine} is currently supported on all U\+N\+I\+X-\/like platforms, including O\+SX and Linux, but is not supported on Windows (its support for Windows is limited to U\+N\+I\+X/\+Linux virtual machines or simulators such as Cygwin or the Linux Subsystem for Windows, as well as any virtual machines that can host a U\+N\+I\+X-\/like guest OS). It can also be built and run in any U\+N\+I\+X-\/like virtual machine (it has been tested extensively with Linux running natively, or as a guest provided by Virtual\+Box). Finally, some functionality is currently limited to Linux only (e.\+g. performance monitoring). During the development of the engine, the decision to include this functionality was not clear; the main reason it was included is to (1) provide some rudimentary performance monitoring support (albeit only in Linux for now), and (2) serve the currently supported Linux-\/only feature as an example to guide further development of this functionality and its extension to other platforms.

\subsubsection*{Building and Build Options}

Before you begin, please make sure to read the notes on dependencies in \href{#technical-notes-and-notes-on-dependencies}{\tt Notes on Dependencies}.

There is a {\ttfamily C\+Make\+Lists.\+txt} supplied with the engine. To build the engine in the {\ttfamily build} directory under, for example, {\ttfamily \mbox{[}G\+L\+OW R\+O\+OT\mbox{]}/inference\+\_\+eingines/build}, execute the command 
\begin{DoxyCode}
cmake -G Ninja -DCMAKE\_BUILD\_TYPE=[Debug/Release] [ADDITIONAL OPTIONS] ../
\end{DoxyCode}
 followed by 
\begin{DoxyCode}
ninja all
\end{DoxyCode}


Of course, {\ttfamily Ninja} can be substituted with any other supported build engine, such as {\ttfamily Unix Makefiles}. The additional options, some required and some optional, are given in the table below. Note also that cross-\/compilation can be facilitated by the {\ttfamily T\+A\+R\+G\+E\+T\+\_\+\+O\+P\+T\+I\+O\+NS} and {\ttfamily T\+A\+R\+G\+E\+T\+\_\+\+L\+I\+N\+K\+E\+R\+\_\+\+F\+L\+A\+GS} options (see below for details). Of course, in this case, when compiling with statically linked bundles, one must make sure that the bundles were also cross-\/compiled for the target platform.

\tabulinesep=1mm
\begin{longtabu} spread 0pt [c]{*{5}{|X[-1]}|}
\hline
\rowcolor{\tableheadbgcolor}\textbf{ Option }&\textbf{ Expected Values }&\textbf{ Description }&\textbf{ Required }&\textbf{ Notes  }\\\cline{1-5}
\endfirsthead
\hline
\endfoot
\hline
\rowcolor{\tableheadbgcolor}\textbf{ Option }&\textbf{ Expected Values }&\textbf{ Description }&\textbf{ Required }&\textbf{ Notes  }\\\cline{1-5}
\endhead
{\ttfamily L\+I\+B\+\_\+\+O\+N\+LY} &O\+N/\+O\+FF &Whether to build the backend library only (without command line application) &No (default is O\+FF) &\\\cline{1-5}
{\ttfamily B\+U\+I\+L\+D\+\_\+\+F\+O\+R\+\_\+\+D\+Y\+N\+A\+M\+I\+C\+\_\+\+L\+I\+N\+K\+A\+GE} &O\+N/\+O\+FF &Whether to build for dynamic linkage (bundle not specified at compile time, but must be specified at runtime and linked in dynamically -- bundle must be dynamically linkable) &No (default is ON) &\\\cline{1-5}
{\ttfamily L\+I\+N\+K\+\_\+\+L\+I\+B\+S\+\_\+\+S\+T\+A\+T\+I\+C\+A\+L\+LY} &O\+N/\+O\+FF &Whether to link everything (e.\+g. {\ttfamily glibc}, {\ttfamily libm}, etc.) statically &No (default is ON) &Static linkage generally inflates object footprint, but may be required when compiling for platforms that may exhibit library version mismatch or might not even carry the required libraries \\\cline{1-5}
{\ttfamily E\+N\+A\+B\+L\+E\+\_\+\+P\+E\+R\+F\+\_\+\+M\+O\+N\+I\+T\+O\+R\+I\+NG} &O\+N/\+O\+FF &Whether to enable performance monitoring &No (default is O\+FF) &Supported on Linux only \\\cline{1-5}
{\ttfamily T\+A\+R\+G\+E\+T\+\_\+\+O\+P\+T\+I\+O\+NS} &A string of additional compile options &Provides the ability to specify additional compiler options (e.\+g. include directories, sysroot, etc -- may be helpful when cross-\/compiling) &No &\\\cline{1-5}
{\ttfamily T\+A\+R\+G\+E\+T\+\_\+\+L\+I\+N\+K\+E\+R\+\_\+\+F\+L\+A\+GS} &A string of additional linker options &Provides the ability to specify additional linker options (e.\+g. libraries, sysroot, etc -- may be helpful when cross-\/compiling) &No &\\\cline{1-5}
{\ttfamily L\+I\+N\+K\+E\+D\+\_\+\+B\+U\+N\+D\+LE} &The bundle path &Specifies the bundle to be compiled in when not compiling for dynamic linkage &Yes only when {\ttfamily B\+U\+I\+L\+D\+\_\+\+F\+O\+R\+\_\+\+D\+Y\+N\+A\+M\+I\+C\+\_\+\+L\+I\+N\+K\+A\+GE} is O\+FF &\\\cline{1-5}
{\ttfamily L\+I\+N\+K\+E\+D\+\_\+\+M\+O\+D\+E\+L\+\_\+\+N\+A\+ME} &The model name &Specifies the model name (the base name for symbols in the specified bundle) &Yes only when {\ttfamily B\+U\+I\+L\+D\+\_\+\+F\+O\+R\+\_\+\+D\+Y\+N\+A\+M\+I\+C\+\_\+\+L\+I\+N\+K\+A\+GE} is O\+FF &\\\cline{1-5}
\end{longtabu}
The produced output is the static library {\ttfamily xinfer}, and, if {\ttfamily L\+I\+B\+\_\+\+O\+N\+LY} is set to O\+FF, the stand-\/alone executable {\ttfamily x-\/infer}.

\subsubsection*{Command Line Options}

The stand-\/alone application accepts many command line options, most of which are required. These are described in the table below, and the documentation can be written to {\ttfamily stdout} with the {\ttfamily -\/-\/help} option. The options may differ depending on whether the application was compiled with a statically-\/linked bundle, or with support for dynamic bundle linkage.

\tabulinesep=1mm
\begin{longtabu} spread 0pt [c]{*{5}{|X[-1]}|}
\hline
\rowcolor{\tableheadbgcolor}\textbf{ Option }&\textbf{ Expected Values }&\textbf{ Description }&\textbf{ Required }&\textbf{ Notes  }\\\cline{1-5}
\endfirsthead
\hline
\endfoot
\hline
\rowcolor{\tableheadbgcolor}\textbf{ Option }&\textbf{ Expected Values }&\textbf{ Description }&\textbf{ Required }&\textbf{ Notes  }\\\cline{1-5}
\endhead
{\ttfamily output} &File name &Output to the specified file instead of {\ttfamily stdout} &No &If omitted, output is written to {\ttfamily stdout} \\\cline{1-5}
{\ttfamily infile} &File name &File containing input(s) in binary format &Yes &\\\cline{1-5}
{\ttfamily intype} &{\ttfamily F32}, {\ttfamily F16}, {\ttfamily I16}, or {\ttfamily I8} &Input data type &Yes &\\\cline{1-5}
{\ttfamily outtype} &{\ttfamily F32}, {\ttfamily F16}, {\ttfamily I16}, or {\ttfamily I8} &Output data type &Yes &\\\cline{1-5}
{\ttfamily inlen} &Integer &Input tensor length (e.\+g. if the tensor is of shape {\ttfamily 2x3x4}, its length is {\ttfamily 2 $\ast$ 3 $\ast$ 4 = 24} &Yes &\\\cline{1-5}
{\ttfamily outlen} &Integer &Output tensor length (e.\+g. if the tensor is of shape {\ttfamily 2x3x4}, its length is {\ttfamily 2 $\ast$ 3 $\ast$ 4 = 24} &Yes &Only single output tensor is currently supported \\\cline{1-5}
{\ttfamily inname} &String (no spaces) &Input tensor name &Yes &Only single named input tensors are currently supported \\\cline{1-5}
{\ttfamily outname} &String (no spaces) &Output tensor name &Yes &Only single named output tensors are currently supported \\\cline{1-5}
{\ttfamily perf} &Optionless Flag &Whether to output performance logs &No (default is NO) &Only supported on Linux, and only available when compiled with the {\ttfamily E\+N\+A\+B\+L\+E\+\_\+\+P\+E\+R\+F\+\_\+\+M\+O\+N\+I\+T\+O\+R\+I\+NG} option \\\cline{1-5}
{\ttfamily perflog} &String (no spaces) &Performance log output filename ({\ttfamily stdout} if omitted) &No &Only supported on Linux, and only available when compiled with the {\ttfamily E\+N\+A\+B\+L\+E\+\_\+\+P\+E\+R\+F\+\_\+\+M\+O\+N\+I\+T\+O\+R\+I\+NG} option \\\cline{1-5}
{\ttfamily model} &String (no spaces) &\hyperlink{struct_model}{Model} name (maximum 128 characters) &Yes &Only available when compiled with the {\ttfamily B\+U\+I\+L\+D\+\_\+\+F\+O\+R\+\_\+\+D\+Y\+N\+A\+M\+I\+C\+\_\+\+L\+I\+N\+K\+A\+GE} option \\\cline{1-5}
\end{longtabu}
The positional arguments are described below.

\tabulinesep=1mm
\begin{longtabu} spread 0pt [c]{*{5}{|X[-1]}|}
\hline
\rowcolor{\tableheadbgcolor}\textbf{ Argument }&\textbf{ Expected Values }&\textbf{ Description }&\textbf{ Required }&\textbf{ Notes  }\\\cline{1-5}
\endfirsthead
\hline
\endfoot
\hline
\rowcolor{\tableheadbgcolor}\textbf{ Argument }&\textbf{ Expected Values }&\textbf{ Description }&\textbf{ Required }&\textbf{ Notes  }\\\cline{1-5}
\endhead
Weights file &String &File name of the weights file &Yes &Specified as the first positional argument if the engine was compiled with a statically linked bundle; otherwise, this is the second positional argument \\\cline{1-5}
Bundle file &String &File name of the dynamically linked bundle &Yes &Specified as the first positional argument if the engine was compiled with support for dynamic bundle linkage; otherwise, not applicable \\\cline{1-5}
\end{longtabu}
\paragraph*{Notes on Input and Output}

Input(s) are given in a single binary file. The layout is a bytestream. The application is able to break the file up into individual inputs knowing the single input length (specified with the {\ttfamily inlen} and {\ttfamily intype}) option. For instance, if the input consists of a single floating 32bit point number, then the input file containing two inputs would consist of 8 bytes, the first four containing the first input, and the last four containing the last input.

The output is composed in a similar way as a bytestream saved into a single binary file. The order of output bytes follows the order of input bytes in the input file.

It is a responsibility of the developer to take care of issues such as byte ordering (e.\+g. big-\/ vs little-\/endianness) in both, the input/output pairs and the weights file.

\subsubsection*{Examples}

For the examples, we shall use the following fully connected feed forward network that has been trained to identify the sine curve on the interval {\ttfamily \mbox{[}0, 2pi\mbox{]}}. Everything above the curve is assigned the class {\ttfamily 0}, while everything below the curve is assigned the class {\ttfamily 1}.


\begin{DoxyCode}
import numpy as np
import torch
import random

class FeedForwardNN(torch.nn.Module):
    def \_\_init\_\_(self, nx, nh, ny, depth, hidden\_activation = torch.nn.ReLU(),
                 output\_activation = torch.nn.Sigmoid(), seed = None, dtype = torch.FloatTensor):
        super().\_\_init\_\_()

        if seed is not None:
            torch.manual\_seed(seed)

        self.params = torch.nn.ParameterList()

        self.layers = [torch.nn.Linear(nx, nh).type(dtype)]
        self.layers.append(hidden\_activation)

        for \_ in range(depth):
            self.layers.append(torch.nn.Linear(nh, nh).type(dtype))
            self.layers.append(hidden\_activation)

        self.layers.append(torch.nn.Linear(nh, ny).type(dtype))
        self.layers.append(output\_activation)

        for layer in self.layers:
            self.params.extend(layer.parameters())

    def forward(self, x):
        y = x
        for layer in self.layers:
            y = layer(y)

        return y

def train(module, loss, X, Y, num\_iters, learning\_rate = 1e-1):
    num\_samples = X.shape[0]
    optimizer = torch.optim.Adam(module.parameters(), lr=learning\_rate)

    for \_ in range(num\_iters):
        optimizer.zero\_grad()
        cost = loss(module(X), Y).sum() / num\_samples 
        cost.backward()
        optimizer.step()

# Number of training and test samples.
m\_train = 10000
m\_test  = 5000

# Generate our data - points in 2D separated by a sine curve.
#
# We set the seed to make sure the experiment is reproduceable.
np.random.seed(1)

x\_train = np.random.rand(m\_train, 2) * 2 - 1
y\_train = np.array([[0 if p[1] < 0.5 * np.sin(2 * np.pi * p[0]) else 1 for p in x\_train]])

# Intentionally misclassify 5% of the points.
m\_train\_missed = m\_train // 20
indices = random.sample(range(0, m\_train), m\_train\_missed)
for index in indices:
    y\_train[0, index] = 0 if y\_train[0, index] != 0 else 1

# Set the structure of our network...
nx    = 2    # Dimension of input vectors.
nh    = 8    # Number of neurons in each hidden layer.
ny    = 1    # Dimension of the output vector.
depth = 2    # Number of hidden layers.
seed  = 0    # The seed for random initialization of the weights.

# Logistic regression loss function for computing the cost.
def logit\_loss(yhat, y):
    return -(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))

ffnn = FeedForwardNN(nx, nh, ny, depth, seed=seed)

# Convert numpy data into torch tensors...
torch\_train\_input = torch.from\_numpy(x\_train).float()
torch\_train\_output = torch.from\_numpy(y\_train.T).float()

# Train the network...
train(ffnn, logit\_loss, torch\_train\_input, torch\_train\_output, 2000)

# Set training mode to False, since at this point we're only interested
# in inference.
ffnn.train(False)

# Create dummy input to go along with our model.
inp = torch.autograd.Variable(torch\_test\_input[:1])

input\_names = ["in"]
output\_names = ["out"]

torch.onnx.export(ffnn, inp, "ffnn.onnx", input\_names=input\_names,
                  output\_names=output\_names, verbose=True, export\_params=True)
\end{DoxyCode}


The sample output from O\+N\+NX is below.


\begin{DoxyCode}
graph(%in : Float(1, 2)
%1 : Float(8, 2)
%2 : Float(8)
%3 : Float(8, 8)
%4 : Float(8)
%5 : Float(8, 8)
%6 : Float(8)
%7 : Float(1, 8)
%8 : Float(1)) \{
%9 : Float(1, 8) = onnx::Gemm[alpha=1, beta=1, transB=1](%in, %1, %2), scope: FeedForwardNN/Linear
%10 : Float(1, 8) = onnx::Relu(%9), scope: FeedForwardNN/ReLU
%11 : Float(1, 8) = onnx::Gemm[alpha=1, beta=1, transB=1](%10, %3, %4), scope: FeedForwardNN/ReLU
%12 : Float(1, 8) = onnx::Relu(%11), scope: FeedForwardNN/ReLU
%13 : Float(1, 8) = onnx::Gemm[alpha=1, beta=1, transB=1](%12, %5, %6), scope: FeedForwardNN/ReLU
%14 : Float(1, 8) = onnx::Relu(%13), scope: FeedForwardNN/ReLU
%15 : Float(1, 1) = onnx::Gemm[alpha=1, beta=1, transB=1](%14, %7, %8), scope: FeedForwardNN/ReLU
%out : Float(1, 1) = onnx::Sigmoid(%15), scope: FeedForwardNN/Sigmoid
return (%out);
\}
\end{DoxyCode}


What is important to note here is that the input tensor name is {\ttfamily in} and the output tensor name is {\ttfamily out}. Also note that the input tensor dimension is {\ttfamily $<$1, 2$>$}, and the output tensor dimension is {\ttfamily $<$1, 1$>$}. The output tensor carries the probability of belonging to class {\ttfamily 0}. This information will be used later.

Now let us produce two versions of bundles\+: one for static linking, and one suitable for dynamic linking. Make sure to change all the relative paths below to reflect your setup (we assume that you have built Glow and the included builders, in particular the {\ttfamily x-\/model-\/builder})

{\bfseries Statically linkable bundle\+:} 
\begin{DoxyCode}
(base) wyessen [~/dts/nn/xgmr/build/debug/osx] $ ./bin/x-model-builder -input-tensor-dims=1,2
       -output-tensor-names=out -model-input-name=in -model=../../../../pytorch\_tuts/ffnn.onnx -emit-bundle=../output/
       -backend=CPU -main-entry-name=ffnn -network-name=ffnn\_static

(base) wyessen [~/dts/nn/xgmr/build/debug/osx] $ ls ../output/
ffnn\_static.o       ffnn\_static.weights

(base) wyessen [~/dts/nn/xgmr/build/debug/osx] $ 
\end{DoxyCode}


{\bfseries Dynamically linkable bundle\+:} 
\begin{DoxyCode}
(base) wyessen [~/dts/nn/xgmr/build/debug/osx] $ ./bin/x-model-builder -input-tensor-dims=1,2
       -output-tensor-names=out -model-input-name=in -model=../../../../pytorch\_tuts/ffnn.onnx -emit-bundle=../output/
       -backend=CPU -main-entry-name=ffnn -network-name=ffnn\_dynamic -llvm-compiler=/usr/local/opt/llvm@8/bin/clang++
       -llvm-compiler-opt=-shared

warning: overriding the module target triple with x86\_64-apple-macosx10.14.0 [-Woverride-module]
1 warning generated.

(base) wyessen [~/dts/nn/xgmr/build/debug/osx] $ ls ../output/
ffnn\_dynamic.bc      ffnn\_dynamic.o       ffnn\_dynamic.weights ffnn\_static.o        ffnn\_static.weights

(base) wyessen [~/dts/nn/xgmr/build/debug/osx] $ 
\end{DoxyCode}


You can safely ignore the generated warning if you are compiling on O\+SX.

\paragraph*{Sample Input}

Let us generate sample input for our inference engine. We\textquotesingle{}ll provide two inputs (four floats in a single binary file, two per input, since input dimension is {\ttfamily $<$1, 2$>$}). Generate it like this\+: 
\begin{DoxyCode}
(base) wyessen [~/dts/nn/xgmr/build/debug/output] $ perl -e 'print pack("f*", 0.123, 0.321, 0.456, 0.654)'
       > sample\_input.dat

(base) wyessen [~/dts/nn/xgmr/build/debug/output] $ od -f sample\_input.dat 

0000000     1.230000e-01    3.210000e-01    4.560000e-01    6.540000e-01
0000020

(base) wyessen [~/dts/nn/xgmr/build/debug/output] $ 
\end{DoxyCode}


\paragraph*{Running with Statically-\/\+Linked Bundles}

Now with the bundles produced, let us go ahead and compile the inference engine. In this case, we are going to specify the bundle at compile time, statically linking it in. We won\textquotesingle{}t statically link all the other required libraries (e.\+g. the math library or the G\+NU argp library).

The command line to prepare the build system is as follows (sample output is also included). 
\begin{DoxyCode}
(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ cmake -G Ninja
       -DLIB\_ONLY=OFF -DBUILD\_FOR\_DYNAMIC\_LINKAGE=OFF -DLINK\_LIBS\_STATICALLY=OFF -DENABLE\_PERF\_MONITORING=OFF
       -DLINKED\_BUNDLE=../../../../../build/debug/output/ffnn\_static.o -DLINKED\_MODEL\_NAME=ffnn ../../../
       -DTARGET\_OPTIONS=-largp

-- The C compiler identification is AppleClang 10.0.1.10010046
-- The CXX compiler identification is AppleClang 10.0.1.10010046
-- Check for working C compiler:
       /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc
-- Check for working C compiler:
       /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler:
       /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++
-- Check for working CXX compiler:
       /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
Will build the library and the executable
-- Configuring done
-- Generating done
-- Build files have been written to:
       /Users/wyessen/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug

(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ 
\end{DoxyCode}


Note that since we\textquotesingle{}re building on O\+SX, we had to explicitly specify {\ttfamily -\/largp} (after installing it with Brew using {\ttfamily brew install argp-\/standalone}).

We can now build with {\ttfamily ninja all}\+: 
\begin{DoxyCode}
(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ ninja all

[1/5] Building C object CMakeFiles/xinfer.dir/x\_perf\_monitor.c.oclang: warning: -largp: 'linker' input
       unused [-Wunused-command-line-argument]
[2/5] Building C object CMakeFiles/xinfer.dir/x\_inference\_lib.c.o
clang: warning: -largp: 'linker' input unused [-Wunused-command-line-argument]
[3/5] Building C object CMakeFiles/x-infer.dir/main.c.o
clang: warning: -largp: 'linker' input unused [-Wunused-command-line-argument]
[4/5] Linking C static library bin/libxinfer.a
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file:
       bin/libxinfer.a(x\_perf\_monitor.c.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file:
       bin/libxinfer.a(x\_perf\_monitor.c.o) has no symbols
[5/5] Linking C executable bin/x-infer

(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ 
\end{DoxyCode}


You can safely ignore the generated warnings. At this point the inference engine has been built. You can verify this as follows.


\begin{DoxyCode}
(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ ./bin/x-infer
       --help

Usage: x-infer [OPTION...] [WEIGHTS FILENAME]

                    Generic Inference Engine                         
-----------------------------------------------------------------------
Dynamic bundle loading: NOT SUPPORTED (bundle has been statically linked)
Performance monitoring: NOT SUPPORTED

x-infer runs inference against the provided Glow bundle. Weights file must be
specified as the first argument. The input file must be specified with
[--infile] (binary file). Input tensor type [-intype], output tensor type
[-outtype], input length [--inlen], output length [--outlen], input tensor name
[-inname], and output tensor name [--outname] must be specified. 

When built with dynamic bundle loading support, bundle must be specified as the
first positional argument, and the weights file as the second. When built with
a bundle statically linked in, dynamic loading is not supported 

Short and long form options are: 

  -i, --infile=FILE          Input from FILE
  -l, --inlen=LEN            Input tensor length (e.g. if the tensor is of
                             shape 2x3x4, its length is 2 * 3 * 4 = 24)
  -L, --outlen=LEN           Output tensor length (e.g. if the tensor is of
                             shape 2x3x4, its length is 2 * 3 * 4 = 24)
  -n, --inname=NAME          Input tensor name NAME
  -N, --outname=NAME         Output tensor name NAME
  -o, --output=FILE          Output to binary FILE instead of standard output
  -t, --intype=TYPE          Input TYPE (one of F32, F16, I16, I8)
  -T, --outtype=TYPE         Output TYPE (one of F32, F16, I16, I8)
  -?, --help                 Give this help list
      --usage                Give a short usage message
  -V, --version              Print program version

Mandatory or optional arguments to long options are also mandatory or optional
for any corresponding short options.

Report bugs to Github Pytorch Glow repository at
https://github.com/pytorch/glow.

(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ 
\end{DoxyCode}


Now let us run on our sample input, saving the output in {\ttfamily output.\+dat}, and then using {\ttfamily od} to interpret the output. 
\begin{DoxyCode}
(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ ./bin/x-infer
       --infile=../../../../../build/debug/output/sample\_input.dat --inlen=2 --outlen=1 --inname=in --outname=save\_out
       --output=./output.dat --intype=F32 --outtype=F32 ../../../../../build/debug/output/ffnn\_static.weights 

(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ od -f output.dat 

0000000     5.850238e-01    9.525478e-01                                
0000010

(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ 
\end{DoxyCode}


Note that we used the output name {\ttfamily save\+\_\+out} instead of {\ttfamily out}. The reason for this is because Glow, upon bundle compilation, adds the prefix {\ttfamily save\+\_\+} to the output tensor name.

\paragraph*{Running with Dynamically-\/\+Loadable Bundles}

First, let us recompile the inference engine with support for dynamic bundle loading\+: 
\begin{DoxyCode}
(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ cmake -G Ninja
       -DLIB\_ONLY=OFF -DBUILD\_FOR\_DYNAMIC\_LINKAGE=ON -DLINK\_LIBS\_STATICALLY=OFF -DENABLE\_PERF\_MONITORING=OFF
       ../../../ -DTARGET\_OPTIONS=-largp

-- The C compiler identification is AppleClang 10.0.1.10010046
-- The CXX compiler identification is AppleClang 10.0.1.10010046
-- Check for working C compiler:
       /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc
-- Check for working C compiler:
       /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler:
       /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++
-- Check for working CXX compiler:
       /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
Will build the library and the executable
Will build the executable for dynamic bundle linkage
-- Configuring done
-- Generating done
-- Build files have been written to:
       /Users/wyessen/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug

(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ 
\end{DoxyCode}


Followed by


\begin{DoxyCode}
(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ ninja all

[1/5] Building C object CMakeFiles/xinfer.dir/x\_perf\_monitor.c.o
clang: warning: -largp: 'linker' input unused [-Wunused-command-line-argument]
[2/5] Building C object CMakeFiles/xinfer.dir/x\_inference\_lib.c.o
clang: warning: -largp: 'linker' input unused [-Wunused-command-line-argument]
[3/5] Building C object CMakeFiles/x-infer.dir/main.c.o
clang: warning: -largp: 'linker' input unused [-Wunused-command-line-argument]
[4/5] Linking C static library bin/libxinfer.a
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file:
       bin/libxinfer.a(x\_perf\_monitor.c.o) has no symbols
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: file:
       bin/libxinfer.a(x\_perf\_monitor.c.o) has no symbols
[5/5] Linking C executable bin/x-infer

(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ 
\end{DoxyCode}


Again, ignore the generated warnings. You can confirm that dynamic linkage is now supported\+:


\begin{DoxyCode}
(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ ./bin/x-infer
       --help

Usage: x-infer [OPTION...] [BUNDLE FILENAME] [WEIGHTS FILENAME]

                    Generic Inference Engine                         
-----------------------------------------------------------------------
Dynamic bundle loading: SUPPORTED
Performance monitoring: NOT SUPPORTED

x-infer runs inference against the provided Glow bundle. Weights file must be
specified as the first argument. The input file must be specified with
[--infile] (binary file). Input tensor type [-intype], output tensor type
[-outtype], input length [--inlen], output length [--outlen], input tensor name
[-inname], and output tensor name [--outname] must be specified. 

When built with dynamic bundle loading support, bundle must be specified as the
first positional argument, and the weights file as the second. When built with
a bundle statically linked in, dynamic loading is not supported 

Short and long form options are: 

  -i, --infile=FILE          Input from FILE
  -l, --inlen=LEN            Input tensor length (e.g. if the tensor is of
                             shape 2x3x4, its length is 2 * 3 * 4 = 24)
  -L, --outlen=LEN           Output tensor length (e.g. if the tensor is of
                             shape 2x3x4, its length is 2 * 3 * 4 = 24)
  -m, --model=NAME           Model name (maximum 128 chars)
  -n, --inname=NAME          Input tensor name NAME
  -N, --outname=NAME         Output tensor name NAME
  -o, --output=FILE          Output to binary FILE instead of standard output
  -t, --intype=TYPE          Input TYPE (one of F32, F16, I16, I8)
  -T, --outtype=TYPE         Output TYPE (one of F32, F16, I16, I8)
  -?, --help                 Give this help list
      --usage                Give a short usage message
  -V, --version              Print program version

Mandatory or optional arguments to long options are also mandatory or optional
for any corresponding short options.

Report bugs to Github Pytorch Glow repository at
https://github.com/pytorch/glow.

(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ 
\end{DoxyCode}


We can now run the inference engine, specifying the bundle we wish to use as the first positional argument, as well as the {\ttfamily -\/-\/model} option specifying the model name\+:


\begin{DoxyCode}
(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ ./bin/x-infer
       --infile=../../../../../build/debug/output/sample\_input.dat --inlen=2 --outlen=1 --inname=in --outname=save\_out
       --output=./output.dat --intype=F32 --outtype=F32 ../../../../../build/debug/output/ffnn\_dynamic.o
       ../../../../../build/debug/output/ffnn\_dynamic.weights --model=ffnn

(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ od -f output.dat 
0000000     5.850238e-01    9.525478e-01                                
0000010

(base) wyessen [~/dts/nn/xgmr/inference\_engines/x-inference-engines/build/native/debug] $ 
\end{DoxyCode}


\paragraph*{Performance Monitoring (Linux Only)}

We can compile with the {\ttfamily -\/\+D\+E\+N\+A\+B\+L\+E\+\_\+\+P\+E\+R\+F\+\_\+\+M\+O\+N\+I\+T\+O\+R\+I\+NG} option (Linux only) to enable performance monitoring. After this, performance will be reported as in the following example. Note that not all V\+Ms support this, and when running in a VM you may not get sensible results. Also, when running natively, it is often the case that you\textquotesingle{}ll need to run with {\ttfamily sudo} to avoid permission errors (due to how performance metrics are gathered, elevated privileges may be required).


\begin{DoxyCode}
wyessen@raspberrypi:~/workbench/glow $ sudo ./bin/x-infer ./bundles/00\_ffnn\_nonquantized/ffnn.o
       ./bundles/00\_ffnn\_nonquantized/ffnn.weights -i ./data/00\_ffnn\_nonquantized/00\_dummy\_input.dat -l 2 -L 1 -m ffnn -n in
       -N save\_out -o output.dat -p -t F32 -T F32

Constant weights size       : 896 bytes
Number of cases             : 1
Number of CPU cycles (x1-e6): 0.014533
\end{DoxyCode}
 Note that C\+PU cycles are counted in millions.

\subsubsection*{Technical Notes and Notes on Dependencies}


\begin{DoxyEnumerate}
\item Currently the engine is written in pure C (C99) and as such can be built and linked with pure C tools.
\item As mentioned above, only U\+N\+I\+X-\/like platforms are currently supported (and for some features, only Linux). In addition, G\+NU argument parsing library (G\+NU Argp) is required when building the stand-\/alone application. Please consult the appropriate documentation for your platform for help with installation. It is provided by default with the G\+NU build tools on many U\+N\+IX(-\/like) platforms, including Linux, and can be installed with {\ttfamily brew install argp-\/standalone} on O\+SX. When compiling on O\+SX, the additional {\ttfamily -\/\+D\+T\+A\+R\+G\+E\+T\+\_\+\+O\+P\+T\+I\+O\+NS=-\/largp} must be specified.
\item Streaming is currently not supported, but this feature is planned.
\item More than one named input, or more than one named output, is currently not supported. This support is planned in the future.
\item Error reporting is rather rudimentary. More detailed error messages are needed. This is planned for the future.
\item It may be possible to figure out input and output tensor lengths from the bundle, so that the {\ttfamily inlen} and {\ttfamily outlen} options can be safely dropped. This is planned for the future. Similarly for input and output types (although this would be more challenging).
\end{DoxyEnumerate}

\subsubsection*{A Note on the Initial Contribution}

The initial contribution of the {\ttfamily x-\/inference-\/engine} (as well as the corresponding documentation) was made as part of the open source contribution initiative by the X\+P\+E\+RI Corporation (xperi.\+com). 