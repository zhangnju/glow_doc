\subsubsection*{Definitions}

\subsubsection*{A}


\begin{DoxyItemize}
\item A\+NN Artificial Neural Network A computational framework based on a synthetic construction of a the biological neural network in brains.
\item A\+OT Ahead Of Time (Compilation) A technique used with J\+IT where some code is compiled during the build and is executed directly rather than compiled when needed.
\end{DoxyItemize}

\subsubsection*{B}


\begin{DoxyItemize}
\item BC Byte Code A representation of the instruction stream encoded to be efficiently interpreted by an executor.
\end{DoxyItemize}

\subsubsection*{C}


\begin{DoxyItemize}
\item C\+NN Convolutional Neural Network A subtype of D\+N\+Ns, using feed-\/forwarding. They are most commonly applied to analysis of images.
\item C\+VP Constant Value Propagation
\end{DoxyItemize}

\subsubsection*{D}


\begin{DoxyItemize}
\item D\+AG Directed Acyclic Graph
\item D\+MA Direct Memory Access Copy data to/from memory without occupying C\+PU time
\item D\+NM Do Not Merge
\item D\+NN Deep Neural Networks A neural network with multiple layers between the input and output layers. These work well with linear and non-\/linear relationships.
\end{DoxyItemize}

\subsubsection*{G}


\begin{DoxyItemize}
\item G\+E\+MM General Matrix Multiply
\item G\+RU Gated Recurrent Unit A gating mechanism for neural networks. They are similar to L\+S\+TM but exhibit better performance characteristics on smaller data sets.
\end{DoxyItemize}

\subsubsection*{I}


\begin{DoxyItemize}
\item IR Intermediate Representation A representation of source code while being converted from the source language to the target language.
\end{DoxyItemize}

\subsubsection*{J}


\begin{DoxyItemize}
\item J\+IT Just In Time (Compilation)
\end{DoxyItemize}

\subsubsection*{L}


\begin{DoxyItemize}
\item L\+G\+TM Looks Good To Me Indicates that the reviewer believes your code to be correct and a positive change. Used to indicate approval for merging the changes.
\item L\+S\+TM Long Short Term Memory Units of R\+N\+Ns consisting of a cell, input gate, output gate, and a forget gate. It is useful to model memory, making it useful for classifying, processing, and predicting over temporal data.
\end{DoxyItemize}

\subsubsection*{M}


\begin{DoxyItemize}
\item M\+LP Multi-\/\+Layer Perceptron A class of feed-\/forward A\+NN, consisting of an input, hidden, and output layers. Excluding the input, the nodes constitute a neuron in the A\+NN. Using non-\/linear activation functions and back-\/propagation allows these networks to distinguish between data with non-\/linear relationships.
\end{DoxyItemize}

\subsubsection*{N}


\begin{DoxyItemize}
\item N\+FC No Functional Change
\item N\+F\+CI No Functional Change Intended
\end{DoxyItemize}

\subsubsection*{R}


\begin{DoxyItemize}
\item R\+A\+UW Replace All Uses With A term commonly used in L\+L\+VM referring to a method that replaces all the uses of one value with another. In Glow, see {\ttfamily Node\+Value\+::replace\+All\+Uses\+Of\+With}.
\item Re\+LU Rectified Linear Unit A unit with a linear activation function in the context of a D\+NN. These are common in computer vision and speech recognition applications.
\item R\+NN Recurrent Neural Network A class of neural networks where the nodes form a D\+AG. It is useful to model temporal dynamic behaviour, making it useful for speech and handwriting recognition.
\end{DoxyItemize}

\subsubsection*{W}


\begin{DoxyItemize}
\item W\+IP Work In Progress Used as a tag to commits that are not ready to be merged. This is often used to mark patches that are being uploaded to test with CI or to get some initial feedback on the changes. 
\end{DoxyItemize}